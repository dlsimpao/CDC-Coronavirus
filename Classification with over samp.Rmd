---
title: "Classification - Over sampling 160"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(timeDate)
library(ggplot2)
library(plotly)
library(hrbrthemes)
#install.packages("hrbrthemes")
#install.packages("tseries")
library(tseries)
#install.packages("ggmap")
library(ggmap)
library(mltools)
library(data.table)
library(caret)
#install.packages("faux")
library(faux)
#install.packages("DataExplorer")
library(DataExplorer)
#install.packages('e1071', dependencies=TRUE)
library(e1071)
#install.packages("pRoc")
library(pROC)



```

# Loading the data
```{r}
cdc_classification_new = read.csv('cdc_filtered_encoded.csv')
cdc_classification_new = cdc_classification_new %>% filter(sex != "Missing")
cdc_classification_new = cdc_classification_new[,2:32]
cdc_classification_new[,c(1,2,4,7:31)] = lapply(cdc_classification_new[,c(1,2,4,7:31)],factor) 

```


```{r}
fitControl <- trainControl(
    method = 'cv',                   # k-fold cross validation
    number = 5,                      # number of folds
    savePredictions = 'final',       # saves predictions for optimal tuning parameter
    classProbs = TRUE,                  # should class probabilities be returned
    summaryFunction=twoClassSummary  # results summary function
) 
```

## Hospitalisation

```{r}
set.seed(100)
hosp_data_yes <- cdc_classification_new %>% filter(hosp_yn == 1)
hosp_data_no <- cdc_classification_new %>% filter(hosp_yn == 0)

samp_hosp_yes <- sample_n(hosp_data_yes,floor(0.50*(nrow(hosp_data_yes))))

samp_row <- floor(0.10*(nrow(cdc_classification_new)) - nrow(samp_hosp_yes))

hosp_random_no <- sample_n(hosp_data_no,samp_row)

hosp_classification <- hosp_random_no %>% bind_rows(samp_hosp_yes)

# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(hosp_classification$hosp_yn, p=0.75, list=FALSE)

# Step 2: Create the training  dataset
trainData <- hosp_classification[trainRowNumbers,]

# Step 3: Create the test dataset
testData <- hosp_classification[-trainRowNumbers,]

hosp_classification %>% filter(hosp_yn == 0)


```


```{r}

train_hosp <- trainData %>% select(c("current_status", "sex", "hosp_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

test_hosp <- testData %>% select(c("current_status", "sex", "hosp_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

#head(train_hosp)



# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_hosp <- dummyVars(hosp_yn ~ . , data=train_hosp)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_hosp <- predict(dummies_model_hosp, newdata = train_hosp)

# # Convert to dataframe
trainData_hosp <- data.frame(trainData_hosp)

trainData_hosp$Hosp <- train_hosp$hosp_yn

# # See the structure of the new dataset
str(trainData_hosp)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_hosp_test <- dummyVars(hosp_yn ~ . , data=test_hosp)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
testData_hosp <- predict(dummies_model_hosp_test, newdata = test_hosp)

# # Convert to dataframe
testData_hosp <- data.frame(testData_hosp)

testData_hosp$Hosp <- test_hosp$hosp_yn

# # See the structure of the new dataset
str(testData_hosp)


```

# FIT CONTROL

```{r}
fitControl <- trainControl(
    method = 'cv',                   # k-fold cross validation
    number = 5,                      # number of folds
    savePredictions = 'final',       # saves predictions for optimal tuning parameter
    classProbs = TRUE,                  # should class probabilities be returned
    summaryFunction=twoClassSummary  # results summary function
) 
```


# Log Reg

```{r}
library(glmnet)
# Set the seed for reproducibility
set.seed(100)


my_glmnet <- getModelInfo("glmnet") %>% magrittr::extract2("glmnet")
my_glmnet$grid <- function (x, y, len = NULL, search = "grid") {
  if (search == "grid") {
    numLev <- if (is.character(y) | is.factor(y)) 
      length(levels(y))
    else NA
    if (!is.na(numLev)) {
      fam <- ifelse(numLev > 2, "multinomial", "binomial")
    }
    else fam <- "gaussian"
    init <- glmnet(as.matrix(x), y, family = fam, nlambda = 52, alpha = 0.5)
    lambda <- unique(init$lambda)
    lambda <- lambda[-c(1, length(lambda))]
    l_seq <- seq(1, length(lambda), length = len) %>% round %>% unique
    lambda <- lambda[l_seq]
    out <- expand.grid(alpha = seq(0.1, 1, length = len), 
                       lambda = lambda)
  }
  else {
    out <- data.frame(alpha = runif(len, min = 0, 1), lambda = 2^runif(len, 
                                                                       min = -10, 3))
  }
  out
}


levels(trainData_hosp$Hosp) <- c("No","Yes")

# Train the model using randomForest and predict on the training data itself.
model_logistic_hosp = train(Hosp ~ ., data=trainData_hosp, method= my_glmnet,trControl = fitControl)

#model_logistic_hosp

#plot(model_logistic, main="Model Accuracies with LR")

varimp_lr_hosp <- varImp(model_logistic_hosp)

varimp_lr_hosp

#plot(varimp_lr, main="Variable Importance with LR")

levels(testData_hosp$Hosp) <- c("No", "Yes")

predicted = predict(model_logistic_hosp,testData_hosp[,c(1:47)])

perf_lr_hosp = confusionMatrix(reference = testData_hosp$Hosp, data = predicted, mode='everything')

perf_lr_hosp


roc_hosp_lr = roc(as.integer(testData_hosp$Hosp),as.integer(predicted))
#plot(roc_hosp_lr)

auc_hosp_lr = auc(roc_hosp_lr)
auc_hosp_lr

#0.7288 = AUC

coef(model_logistic_hosp$finalModel,model_logistic_hosp$bestTune$lambda)



```


#### SVM 

```{r}

# Training the model on training data
levels(trainData_hosp$Hosp) <- c("No", "Yes")

model_svm_hosp =train(Hosp ~ .,
    data = trainData_hosp,
    method = "svmLinear",
    preProc = NULL,
    trControl = fitControl)

model_svm_hosp



varimp_svm_hosp <- varImp(model_svm_hosp)

varimp_svm_hosp

#plot(varimp_svm, main="Variable Importance with SVM")

levels(testData_hosp$Hosp) <- c("No", "Yes")

predicted = predict(model_svm_hosp,testData_hosp[,c(1:47)])

perf_svm_hosp = confusionMatrix(reference = testData_hosp$Hosp, data = predicted, mode='everything')

perf_svm_hosp

roc_hosp_svm = roc(as.integer(testData_hosp$Hosp),as.integer(predicted))
#plot(roc_hosp_svm)

auc_hosp_svm = auc(roc_hosp_svm)
auc_hosp_svm

#auc = 0.7265


coefs_hosp <- model_svm_hosp$finalModel@coef[[1]]
mat_hosp <- model_svm_hosp$finalModel@xmatrix[[1]]

w_hosp <- coefs_hosp %*% mat_hosp

w_hosp

```


#### Random Forest


```{r,warning=FALSE}
# Training the model on training data
levels(trainData_hosp$Hosp) <- c("No", "Yes")
model_rf_hosp = train(Hosp ~ ., data=trainData_hosp, method='rf', tuneLength= 10, trControl = fitControl)
#model_rf

# Testing it on test data

varimp_rf_hosp <- varImp(model_rf_hosp)

varimp_rf_hosp


#plot(varimp_rf, main="Variable Importance with Random Forest")

levels(testData_hosp$Hosp) <- c("No", "Yes")

predicted = predict(model_rf_hosp,testData_hosp[,c(1:47)])

perf_rf_hosp = confusionMatrix(reference = testData_hosp$Hosp, data = predicted, mode='everything')

perf_rf_hosp



roc_hosp_rf = roc(as.integer(testData_hosp$Hosp),as.integer(predicted))
plot(roc_hosp_rf)

auc_hosp_rf = auc(roc_hosp_rf)
auc_hosp_rf



# auc = 0.7432

```

```{r}
save(model_logistic_hosp,model_svm_hosp,model_rf_hosp,varimp_lr_hosp,varimp_svm_hosp,varimp_rf_hosp,perf_lr_hosp,perf_svm_hosp,perf_rf_hosp,file = "HospModels.RData")
```



## For classification of ICU (including only symptoms)

### Testing on subset


```{r}
set.seed(100)
icu_data_yes <- cdc_classification_new %>% filter(icu_yn == 1)
icu_data_no <- cdc_classification_new %>% filter(icu_yn == 0)

samp_row <- floor(0.10*(nrow(cdc_classification_new)) - nrow(icu_data_yes))

icu_random_no <- sample_n(icu_data_no,samp_row)

icu_classification <- icu_random_no %>% bind_rows(icu_data_yes)

# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(icu_classification$icu_yn, p=0.75, list=FALSE)

# Step 2: Create the training  dataset
trainData <- icu_classification[trainRowNumbers,]

# Step 3: Create the test dataset
testData <- icu_classification[-trainRowNumbers,]




```




```{r}

train_icu <- trainData %>% select(c("current_status", "sex", "icu_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

test_icu <- testData %>% select(c("current_status", "sex", "icu_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

#head(train_icu)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_icu <- dummyVars(icu_yn ~ . , data=train_icu)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_icu <- predict(dummies_model_icu, newdata = train_icu)

# # Convert to dataframe
trainData_icu <- data.frame(trainData_icu)

trainData_icu$ICU <- train_icu$icu_yn

# # See the structure of the new dataset
str(trainData_icu)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_icu_test <- dummyVars(icu_yn ~ . , data=test_icu)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
testData_icu <- predict(dummies_model_icu_test, newdata = test_icu)

# # Convert to dataframe
testData_icu <- data.frame(testData_icu)

testData_icu$ICU <- test_icu$icu_yn

# # See the structure of the new dataset
str(testData_icu)


```




# Log Reg

```{r}
library(glmnet)
# Set the seed for reproducibility
set.seed(100)


my_glmnet <- getModelInfo("glmnet") %>% magrittr::extract2("glmnet")
my_glmnet$grid <- function (x, y, len = NULL, search = "grid") {
  if (search == "grid") {
    numLev <- if (is.character(y) | is.factor(y)) 
      length(levels(y))
    else NA
    if (!is.na(numLev)) {
      fam <- ifelse(numLev > 2, "multinomial", "binomial")
    }
    else fam <- "gaussian"
    init <- glmnet(as.matrix(x), y, family = fam, nlambda = 52, alpha = 0.5)
    lambda <- unique(init$lambda)
    lambda <- lambda[-c(1, length(lambda))]
    l_seq <- seq(1, length(lambda), length = len) %>% round %>% unique
    lambda <- lambda[l_seq]
    out <- expand.grid(alpha = seq(0.1, 1, length = len), 
                       lambda = lambda)
  }
  else {
    out <- data.frame(alpha = runif(len, min = 0, 1), lambda = 2^runif(len, 
                                                                       min = -10, 3))
  }
  out
}

levels(trainData_icu$ICU) <- c("No","Yes")

model_logistic_icu = train(ICU ~ ., data=trainData_icu, method= my_glmnet,trControl = fitControl)

#plot(model_logistic, main="Model Accuracies with LR")

varimp_lr_icu <- varImp(model_logistic_icu)

varimp_lr_icu

#plot(varimp_lr, main="Variable Importance with LR")

levels(testData_icu$ICU) <- c("No", "Yes")

predicted = predict(model_logistic_icu,testData_icu[,c(1:47)])

perf_lr_icu = confusionMatrix(reference = testData_icu$ICU, data = predicted, mode='everything')

perf_lr_icu


roc_icu_lr = roc(as.integer(testData_icu$ICU),as.integer(predicted))
#plot(roc_hosp_lr)

auc_icu_lr = auc(roc_icu_lr)
auc_icu_lr

#accuracy = 0.
# auc = 0.8409
# Sensitivity = 0.9710
# Specificity = 0.7108
# F1 = 0.9465
# Recall = 0.9710



coef(model_logistic_icu$finalModel,model_logistic_icu$bestTune$lambda)

#model_logistic_icu

```


#### SVM 

```{r}
set.seed(100)

# Training the model on training data
levels(trainData_icu$ICU) <- c("No", "Yes")

model_svm_icu =train(ICU ~ .,
    data = trainData_icu,
    method = "svmLinear",
    preProc = NULL,
    trControl = fitControl)


# Testing it on test data

varimp_svm_icu <- varImp(model_svm_icu)

varimp_svm_icu

#plot(varimp_svm, main="Variable Importance with SVM")

levels(testData_icu$ICU) <- c("No", "Yes")

predicted = predict(model_svm_icu,testData_icu[,c(1:47)])

perf_svm_icu = confusionMatrix(reference = testData_icu$ICU, data = predicted, mode='everything')

perf_svm_icu


roc_icu_svm = roc(as.integer(testData_icu$ICU),as.integer(predicted))
#plot(roc_hosp_lr)

auc_icu_svm = auc(roc_icu_svm)
auc_icu_svm




# auc = 0.8148
# Sensitivity = 0.9707
# Spe = 0.6588
# F1 = 0.9307
# Recall = 0.9707

coefs_icu <- model_svm_icu$finalModel@coef[[1]]
mat_icu <- model_svm_icu$finalModel@xmatrix[[1]]

w_icu <- coefs_icu %*% mat_icu

w_icu

```


#### Random Forest


```{r}
# Training the model on training data
levels(trainData_icu$ICU) <- c("No", "Yes")
model_rf_icu = train(ICU ~ ., data=trainData_icu, method='rf', tuneLength= 10, trControl = fitControl)
#model_rf

# Testing it on test data

varimp_rf_icu <- varImp(model_rf_icu)

varimp_rf_icu

#plot(varimp_rf, main="Variable Importance with Random Forest")

levels(testData_icu$ICU) <- c("No", "Yes")

predicted = predict(model_rf_icu,testData_icu[,c(1:47)])

perf_rf_icu = confusionMatrix(reference = testData_icu$ICU, data = predicted, mode='everything')

perf_rf_icu

roc_icu_rf = roc(as.integer(testData_icu$ICU),as.integer(predicted))

plot(roc_icu_rf)

auc_icu_rf = auc(roc_icu_rf)
auc_icu_rf

# auc = 0.9258
# accuracy = 0.9482
# Sensitivity = 0.9656
# Specificity = 0.8860
# F1 = 0.9668
# Recall = 0.9656
```



```{r}

save(model_logistic_icu,model_svm_icu,model_rf_icu,varimp_lr_icu,varimp_svm_icu,varimp_rf_icu,perf_lr_icu,perf_svm_icu,perf_rf_icu,file = "ICUModels.RData")

```

### Ventilator (With only symptoms)


```{r}
set.seed(100)
mechvent_data_yes <- cdc_classification_new %>% filter(mechvent_yn == 1)
mechvent_data_no <- cdc_classification_new %>% filter(mechvent_yn == 0)

samp_row <- floor(0.10*(nrow(cdc_classification_new)) - nrow(mechvent_data_yes))

mechvent_random_no <- sample_n(mechvent_data_no,samp_row)

mechvent_classification <- mechvent_random_no %>% bind_rows(mechvent_data_yes)

# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(mechvent_classification$mechvent_yn, p=0.75, list=FALSE)

# Step 2: Create the training  dataset
trainData <- mechvent_classification[trainRowNumbers,]

# Step 3: Create the test dataset
testData <- mechvent_classification[-trainRowNumbers,]




```


```{r}

train_vent <- trainData %>% select(c("current_status", "sex", "mechvent_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

test_vent <- testData %>% select(c("current_status", "sex", "mechvent_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

#head(train_vent)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_vent <- dummyVars(mechvent_yn ~ . , data=train_vent)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_vent <- predict(dummies_model_vent, newdata = train_vent)

# # Convert to dataframe
trainData_vent <- data.frame(trainData_vent)

trainData_vent$Ventilator <- train_vent$mechvent_yn

# # See the structure of the new dataset
str(trainData_vent)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_vent_test <- dummyVars(mechvent_yn ~ . , data=test_vent)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
testData_vent <- predict(dummies_model_vent_test, newdata = test_vent)

# # Convert to dataframe
testData_vent <- data.frame(testData_vent)

testData_vent$Ventilator <- test_vent$mechvent_yn

# # See the structure of the new dataset
str(testData_vent)


```


### Log Reg

```{r}
library(glmnet)
# Set the seed for reproducibility
set.seed(100)


my_glmnet <- getModelInfo("glmnet") %>% magrittr::extract2("glmnet")
my_glmnet$grid <- function (x, y, len = NULL, search = "grid") {
  if (search == "grid") {
    numLev <- if (is.character(y) | is.factor(y)) 
      length(levels(y))
    else NA
    if (!is.na(numLev)) {
      fam <- ifelse(numLev > 2, "multinomial", "binomial")
    }
    else fam <- "gaussian"
    init <- glmnet(as.matrix(x), y, family = fam, nlambda = 52, alpha = 0.5)
    lambda <- unique(init$lambda)
    lambda <- lambda[-c(1, length(lambda))]
    l_seq <- seq(1, length(lambda), length = len) %>% round %>% unique
    lambda <- lambda[l_seq]
    out <- expand.grid(alpha = seq(0.1, 1, length = len), 
                       lambda = lambda)
  }
  else {
    out <- data.frame(alpha = runif(len, min = 0, 1), lambda = 2^runif(len, 
                                                                       min = -10, 3))
  }
  out
}


levels(trainData_vent$Ventilator) <- c("No","Yes")

# Train the model using randomForest and predict on the training data itself.
model_logistic_vent = train(Ventilator ~ ., data=trainData_vent, method= my_glmnet, trControl = fitControl )

#plot(model_logistic_vent, main="Model Accuracies with LR")

varimp_lr_vent <- varImp(model_logistic_vent)

varimp_lr_vent

#plot(varimp_lr, main="Variable Importance with LR")

levels(testData_vent$Ventilator) <- c("No","Yes")

predicted = predict(model_logistic_vent,testData_vent[,c(1:47)])

perf_lr_vent = confusionMatrix(reference = testData_vent$Ventilator, data = predicted, mode='everything')

perf_lr_vent


roc_vent_lr = roc(as.integer(testData_vent$Ventilator),as.integer(predicted))
#plot(roc_hosp_lr)

auc_vent_lr = auc(roc_vent_lr)
auc_vent_lr

# auc = 0.6805



coef(model_logistic_vent$finalModel,model_logistic_vent$bestTune$lambda)


```


#### SVM 

```{r}

set.seed(100)

# Training the model on training data
levels(trainData_vent$Ventilator) <- c("No", "Yes")
model_svm_vent =train(Ventilator ~ .,
    data = trainData_vent,
    method = "svmLinear",
    preProc = NULL,
    trControl = fitControl)

#model_svmRadial

# Testing it on test data

varimp_svm_vent <- varImp(model_svm_vent)

varimp_svm_vent

#plot(varimp_svm, main="Variable Importance with SVM")

levels(testData_vent$Ventilator) <- c("No", "Yes")

predicted = predict(model_svm_vent,testData_vent[,c(1:47)])

perf_svm_vent = confusionMatrix(reference = testData_vent$Ventilator, data = predicted, mode='everything')

perf_svm_vent


roc_vent_svm = roc(as.integer(testData_vent$Ventilator),as.integer(predicted))
#plot(roc_hosp_lr)

auc_vent_svm = auc(roc_vent_svm)
auc_vent_svm




# auc = 0.6354


coefs_vent <- model_svm_vent$finalModel@coef[[1]]
mat_vent <- model_svm_vent$finalModel@xmatrix[[1]]

w_vent <- coefs_vent %*% mat_vent

w_vent


```


#### Random Forest


```{r}
set.seed(100)

# Training the model on training data
levels(trainData_vent$Ventilator) <- c("No", "Yes")
model_rf_vent = train(Ventilator ~ ., data=trainData_vent, method='rf', tuneLength= 10, trControl = fitControl)
#model_rf

# Testing it on test data

varimp_rf_vent <- varImp(model_rf_vent)

varimp_rf_vent

#plot(varimp_rf, main="Variable Importance with Random Forest")

levels(testData_vent$Ventilator) <- c("No", "Yes")

predicted = predict(model_rf_vent,testData_vent[,c(1:47)])

perf_rf_vent = confusionMatrix(reference = testData_vent$Ventilator, data = predicted, mode='everything')

perf_rf_vent

roc_vent_rf = roc(as.integer(testData_vent$Ventilator),as.integer(predicted))
#plot(roc_hosp_lr)

auc_vent_rf = auc(roc_vent_rf)
auc_vent_rf

# auc = 0.9508

```


```{r}

save(model_logistic_vent,model_svm_vent,model_rf_vent,varimp_lr_vent,varimp_svm_vent,varimp_rf_vent,perf_lr_vent,perf_svm_vent,perf_rf_vent,file = "Vent1Models.RData")

```

## Ventilator (with symptoms and ICU info)

```{r}

train_vent2 <- trainData %>% select(c("current_status", "sex", "mechvent_yn", "hc_work_yn", "icu_yn", "pna_yn", "abxchest_yn" ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",    "cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

test_vent2 <- testData %>% select(c("current_status", "sex", "mechvent_yn", "hc_work_yn", "icu_yn", "pna_yn", "abxchest_yn" ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",   "cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

#head(train_vent2)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_vent2 <- dummyVars(mechvent_yn ~ . , data=train_vent2)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_vent2 <- predict(dummies_model_vent2, newdata = train_vent2)

# # Convert to dataframe
trainData_vent2 <- data.frame(trainData_vent2)

trainData_vent2$Ventilator <- train_vent2$mechvent_yn

# # See the structure of the new dataset
str(trainData_vent2)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_vent2_test <- dummyVars(mechvent_yn ~ . , data=test_vent2)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
testData_vent2 <- predict(dummies_model_vent2_test, newdata = test_vent2)

# # Convert to dataframe
testData_vent2 <- data.frame(testData_vent2)

testData_vent2$Ventilator <- test_vent2$mechvent_yn

# # See the structure of the new dataset
str(testData_vent2)


```

### Log Reg

```{r}
library(glmnet)
# Set the seed for reproducibility
set.seed(100)


my_glmnet <- getModelInfo("glmnet") %>% magrittr::extract2("glmnet")
my_glmnet$grid <- function (x, y, len = NULL, search = "grid") {
  if (search == "grid") {
    numLev <- if (is.character(y) | is.factor(y)) 
      length(levels(y))
    else NA
    if (!is.na(numLev)) {
      fam <- ifelse(numLev > 2, "multinomial", "binomial")
    }
    else fam <- "gaussian"
    init <- glmnet(as.matrix(x), y, family = fam, nlambda = 52, alpha = 0.5)
    lambda <- unique(init$lambda)
    lambda <- lambda[-c(1, length(lambda))]
    l_seq <- seq(1, length(lambda), length = len) %>% round %>% unique
    lambda <- lambda[l_seq]
    out <- expand.grid(alpha = seq(0.1, 1, length = len), 
                       lambda = lambda)
  }
  else {
    out <- data.frame(alpha = runif(len, min = 0, 1), lambda = 2^runif(len, 
                                                                       min = -10, 3))
  }
  out
}


levels(trainData_vent2$Ventilator) <- c("No","Yes")

# Train the model using randomForest and predict on the training data itself.
model_logistic_vent2 = train(Ventilator ~ ., data=trainData_vent2, method= my_glmnet, trControl = fitControl )

#plot(model_logistic_vent, main="Model Accuracies with LR")

varimp_lr_vent2 <- varImp(model_logistic_vent2)

varimp_lr_vent2

#plot(varimp_lr, main="Variable Importance with LR")

levels(testData_vent2$Ventilator) <- c("No","Yes")

predicted = predict(model_logistic_vent2,testData_vent2[,c(1:49)])

perf_lr_vent2 = confusionMatrix(reference = testData_vent2$Ventilator, data = predicted, mode='everything')

perf_lr_vent2


roc_vent2_lr = roc(as.integer(testData_vent2$Ventilator),as.integer(predicted))
#plot(roc_hosp_lr)

auc_vent2_lr = auc(roc_vent2_lr)
auc_vent2_lr

# auc = 0.7



coef(model_logistic_vent2$finalModel,model_logistic_vent2$bestTune$lambda)


```


#### SVM 

```{r}

set.seed(100)

# Training the model on training data
levels(trainData_vent$Ventilator) <- c("No", "Yes")
model_svm_vent2 =train(Ventilator ~ .,
    data = trainData_vent2,
    method = "svmLinear",
    preProc = NULL,
    trControl = fitControl)

#model_svmRadial

# Testing it on test data

varimp_svm_vent2 <- varImp(model_svm_vent2)

varimp_svm_vent2

#plot(varimp_svm, main="Variable Importance with SVM")

levels(testData_vent2$Ventilator) <- c("No", "Yes")

predicted = predict(model_svm_vent2,testData_vent2[,c(1:49)])

perf_svm_vent2 = confusionMatrix(reference = testData_vent2$Ventilator, data = predicted, mode='everything')

perf_svm_vent2


roc_vent2_svm = roc(as.integer(testData_vent2$Ventilator),as.integer(predicted))
#plot(roc_hosp_lr)

auc_vent2_svm = auc(roc_vent2_svm)
auc_vent2_svm




# auc = 0.7034


coefs_vent2 <- model_svm_vent2$finalModel@coef[[1]]
mat_vent2 <- model_svm_vent2$finalModel@xmatrix[[1]]

w_vent2 <- coefs_vent2 %*% mat_vent2

w_vent2


```


#### Random Forest


```{r}
set.seed(100)

# Training the model on training data
levels(trainData_vent2$Ventilator) <- c("No", "Yes")
model_rf_vent2 = train(Ventilator ~ ., data=trainData_vent2, method='rf', tuneLength= 10, trControl = fitControl)
#model_rf

# Testing it on test data

varimp_rf_vent2 <- varImp(model_rf_vent2)

varimp_rf_vent2

#plot(varimp_rf, main="Variable Importance with Random Forest")

levels(testData_vent2$Ventilator) <- c("No", "Yes")

predicted = predict(model_rf_vent2,testData_vent2[,c(1:49)])

perf_rf_vent2 = confusionMatrix(reference = testData_vent2$Ventilator, data = predicted, mode='everything')

perf_rf_vent2

roc_vent2_rf = roc(as.integer(testData_vent2$Ventilator),as.integer(predicted))
#plot(roc_hosp_lr)

auc_vent2_rf = auc(roc_vent2_rf)
auc_vent2_rf

# auc = 0.726

```


```{r}

save(model_logistic_vent2,model_svm_vent2,model_rf_vent2,varimp_lr_vent2,varimp_svm_vent2,varimp_rf_vent2,perf_lr_vent2,perf_svm_vent2,perf_rf_vent2,file = "Vent2Models.RData")

```

# Mortality (with symptoms only)

```{r}
set.seed(100)
mort_data_yes <- cdc_classification_new %>% filter(death_yn == 1)
mort_data_no <- cdc_classification_new %>% filter(death_yn == 0)

samp_row <- floor(0.10*(nrow(cdc_classification_new)) - nrow(mort_data_yes))

mort_random_no <- sample_n(mort_data_no,samp_row)

mort_classification <- mort_random_no %>% bind_rows(mort_data_yes)

# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(mort_classification$death_yn, p=0.75, list=FALSE)

# Step 2: Create the training  dataset
trainData <- mort_classification[trainRowNumbers,]

# Step 3: Create the test dataset
testData <- mort_classification[-trainRowNumbers,]




```


```{r}

train_mort <- trainData %>% select(c("current_status", "sex", "death_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

test_mort <- testData %>% select(c("current_status", "sex", "death_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

#head(train_icu)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_mort <- dummyVars(death_yn ~ . , data=train_mort)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mort <- predict(dummies_model_mort, newdata = train_mort)

# # Convert to dataframe
trainData_mort <- data.frame(trainData_mort)

trainData_mort$Mortality <- train_mort$death_yn

# # See the structure of the new dataset
str(trainData_mort)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_mort_test <- dummyVars(death_yn ~ . , data=test_mort)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
testData_mort <- predict(dummies_model_mort_test, newdata = test_mort)

# # Convert to dataframe
testData_mort <- data.frame(testData_mort)

testData_mort$Mortality <- test_mort$death_yn

# # See the structure of the new dataset
str(testData_mort)


```



# Logistic Reg

```{r}
library(glmnet)
# Set the seed for reproducibility
set.seed(100)


my_glmnet <- getModelInfo("glmnet") %>% magrittr::extract2("glmnet")
my_glmnet$grid <- function (x, y, len = NULL, search = "grid") {
  if (search == "grid") {
    numLev <- if (is.character(y) | is.factor(y)) 
      length(levels(y))
    else NA
    if (!is.na(numLev)) {
      fam <- ifelse(numLev > 2, "multinomial", "binomial")
    }
    else fam <- "gaussian"
    init <- glmnet(as.matrix(x), y, family = fam, nlambda = 52, alpha = 0.5)
    lambda <- unique(init$lambda)
    lambda <- lambda[-c(1, length(lambda))]
    l_seq <- seq(1, length(lambda), length = len) %>% round %>% unique
    lambda <- lambda[l_seq]
    out <- expand.grid(alpha = seq(0.1, 1, length = len), 
                       lambda = lambda)
  }
  else {
    out <- data.frame(alpha = runif(len, min = 0, 1), lambda = 2^runif(len, 
                                                                       min = -10, 3))
  }
  out
}


levels(trainData_mort$Mortality) <- c("No", "Yes")

# Train the model using randomForest and predict on the training data itself.
model_logistic_mort = train(Mortality ~ ., data=trainData_mort, method= my_glmnet,trControl = fitControl)

#plot(model_logistic, main="Model Accuracies with LR")

varimp_lr_mort <- varImp(model_logistic_mort)

varimp_lr_mort

#plot(varimp_lr, main="Variable Importance with LR")

levels(testData_mort$Mortality) <- c("No", "Yes")

predicted = predict(model_logistic_mort,testData_mort[,c(1:47)])

perf_lr_mort = confusionMatrix(reference = testData_mort$Mortality, data = predicted, mode='everything')

perf_lr_mort

roc_mort_lr = roc(as.integer(testData_mort$Mortality),as.integer(predicted))
#plot(roc_hosp_lr)

auc_mort_lr = auc(roc_mort_lr)
auc_mort_lr

# auc = 0.6395



coef(model_logistic_mort$finalModel,model_logistic_mort$bestTune$lambda)




```

#### SVM 

```{r}

# Training the model on training data
levels(trainData_mort$Mortality) <- c("No", "Yes")
model_svm_mort =train(Mortality ~ .,
    data = trainData_mort,
    method = "svmLinear",
    preProc = NULL,
    trControl = fitControl)
#model_svmRadial

# Testing it on test data

varimp_svm_mort <- varImp(model_svm_mort)

varimp_svm_mort

#plot(varimp_svm, main="Variable Importance with SVM")

levels(testData_mort$Mortality) <- c("No", "Yes")

predicted = predict(model_svm_mort,testData_mort[,c(1:47)])

perf_svm_mort = confusionMatrix(reference = testData_mort$Mortality, data = predicted, mode='everything')

perf_svm_mort


roc_mort_svm = roc(as.integer(testData_mort$Mortality),as.integer(predicted))
#plot(roc_hosp_lr)

auc_mort_svm = auc(roc_mort_svm)
auc_mort_svm

# auc = 0.5637


coefs_mort <- model_svm_mort$finalModel@coef[[1]]
mat_mort <- model_svm_mort$finalModel@xmatrix[[1]]

w_mort <- coefs_mort %*% mat_mort

w_mort



```


#### Random Forest


```{r}
# Training the model on training data
levels(trainData_mort$Mortality) <- c("No", "Yes")
model_rf_mort = train(Mortality ~ ., data=trainData_mort, method='rf', tuneLength= 10, trControl = fitControl)
#model_rf

# Testing it on test data

varimp_rf_mort <- varImp(model_rf_mort)

varimp_rf_mort

#plot(varimp_rf, main="Variable Importance with Random Forest")

levels(testData_mort$Mortality) <- c("No", "Yes")

predicted = predict(model_rf_mort,testData_mort[,c(1:47)])

perf_rf_mort = confusionMatrix(reference = testData_mort$Mortality, data = predicted, mode='everything')

perf_rf_mort

roc_mort_rf = roc(as.integer(testData_mort$Mortality),as.integer(predicted))
#plot(roc_hosp_lr)

auc_mort_rf = auc(roc_mort_rf)
auc_mort_rf

# auc = 0.6128


```


```{r}

save(model_logistic_mort,model_svm_mort,model_rf_mort,varimp_lr_mort,varimp_svm_mort,varimp_rf_mort,perf_lr_mort,perf_svm_mort,perf_rf_mort,file = "Mort1Models.RData")

```

### Mortality (With symptoms, hospitalization, ICU and ventilator)

```{r}

train_mort2 <- trainData %>% select(c("current_status", "sex","hosp_yn","icu_yn","mechvent_yn" ,"death_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn","cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

test_mort2 <- testData %>% select(c("current_status", "sex", "hosp_yn","icu_yn","mechvent_yn" , "death_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn", "cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

#head(train_mort2)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_mort2 <- dummyVars(death_yn ~ . , data=train_mort2)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mort2 <- predict(dummies_model_mort2, newdata = train_mort2)

# # Convert to dataframe
trainData_mort2 <- data.frame(trainData_mort2)

trainData_mort2$Mortality <- train_mort2$death_yn

# # See the structure of the new dataset
str(trainData_mort2)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_mort2_test <- dummyVars(death_yn ~ . , data=test_mort2)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
testData_mort2 <- predict(dummies_model_mort2_test, newdata = test_mort2)

# # Convert to dataframe
testData_mort2 <- data.frame(testData_mort2)

testData_mort2$Mortality <- test_mort2$death_yn

# # See the structure of the new dataset
str(testData_mort2)


```


# Logistic Reg

```{r}
library(glmnet)
# Set the seed for reproducibility
set.seed(100)


my_glmnet <- getModelInfo("glmnet") %>% magrittr::extract2("glmnet")
my_glmnet$grid <- function (x, y, len = NULL, search = "grid") {
  if (search == "grid") {
    numLev <- if (is.character(y) | is.factor(y)) 
      length(levels(y))
    else NA
    if (!is.na(numLev)) {
      fam <- ifelse(numLev > 2, "multinomial", "binomial")
    }
    else fam <- "gaussian"
    init <- glmnet(as.matrix(x), y, family = fam, nlambda = 52, alpha = 0.5)
    lambda <- unique(init$lambda)
    lambda <- lambda[-c(1, length(lambda))]
    l_seq <- seq(1, length(lambda), length = len) %>% round %>% unique
    lambda <- lambda[l_seq]
    out <- expand.grid(alpha = seq(0.1, 1, length = len), 
                       lambda = lambda)
  }
  else {
    out <- data.frame(alpha = runif(len, min = 0, 1), lambda = 2^runif(len, 
                                                                       min = -10, 3))
  }
  out
}


levels(trainData_mort2$Mortality) <- c("No", "Yes")

# Train the model using randomForest and predict on the training data itself.
model_logistic_mort2 = train(Mortality ~ ., data=trainData_mort2, method= my_glmnet,trControl = fitControl)

#plot(model_logistic, main="Model Accuracies with LR")

varimp_lr_mort2 <- varImp(model_logistic_mort2)

varimp_lr_mort2

#plot(varimp_lr, main="Variable Importance with LR")

levels(testData_mort2$Mortality) <- c("No", "Yes")

predicted = predict(model_logistic_mort2,testData_mort2[,c(1:53)])

perf_lr_mort2 = confusionMatrix(reference = testData_mort2$Mortality, data = predicted, mode='everything')

perf_lr_mort2

roc_mort2_lr = roc(as.integer(testData_mort2$Mortality),as.integer(predicted))
#plot(roc_hosp_lr)

auc_mort2_lr = auc(roc_mort2_lr)
auc_mort2_lr

# auc = 0.6654



coef(model_logistic_mort2$finalModel,model_logistic_mort2$bestTune$lambda)




```

#### SVM 

```{r}
set.seed(100)

# Training the model on training data
levels(trainData_mort2$Mortality) <- c("No", "Yes")
model_svm_mort2 =train(Mortality ~ .,
    data = trainData_mort2,
    method = "svmLinear",
    preProc = NULL,
    trControl = fitControl)
#model_svmRadial

# Testing it on test data

varimp_svm_mort2 <- varImp(model_svm_mort2)

varimp_svm_mort2

#plot(varimp_svm, main="Variable Importance with SVM")

levels(testData_mort2$Mortality) <- c("No", "Yes")

predicted = predict(model_svm_mort2,testData_mort2[,c(1:53)])

perf_svm_mort2 = confusionMatrix(reference = testData_mort2$Mortality, data = predicted, mode='everything')

perf_svm_mort2


roc_mort2_svm = roc(as.integer(testData_mort2$Mortality),as.integer(predicted))
#plot(roc_hosp_lr)

auc_mort2_svm = auc(roc_mort2_svm)
auc_mort2_svm

# auc = 0.6145


coefs_mort2 <- model_svm_mort2$finalModel@coef[[1]]
mat_mort2 <- model_svm_mort2$finalModel@xmatrix[[1]]

w_mort2 <- coefs_mort2 %*% mat_mort2

w_mort2



```


#### Random Forest


```{r}
# Training the model on training data
levels(trainData_mort2$Mortality) <- c("No", "Yes")
model_rf_mort2 = train(Mortality ~ ., data=trainData_mort2, method='rf', tuneLength= 10, trControl = fitControl)
#model_rf

# Testing it on test data

varimp_rf_mort2 <- varImp(model_rf_mort2)

varimp_rf_mort2

#plot(varimp_rf, main="Variable Importance with Random Forest")

levels(testData_mort2$Mortality) <- c("No", "Yes")

predicted = predict(model_rf_mort2,testData_mort2[,c(1:53)])

perf_rf_mort2 = confusionMatrix(reference = testData_mort2$Mortality, data = predicted, mode='everything')

perf_rf_mort2

roc_mort2_rf = roc(as.integer(testData_mort2$Mortality),as.integer(predicted))
#plot(roc_hosp_lr)

auc_mort2_rf = auc(roc_mort2_rf)
auc_mort2_rf

# auc = 0.6525


```


```{r}
save(model_logistic_mort2,model_svm_mort2,model_rf_mort2,varimp_lr_mort2,varimp_svm_mort2,varimp_rf_mort2,perf_lr_mort2,perf_svm_mort2,perf_rf_mort2,file = "Mort2Models.RData")

```



### Saving best models

```{r}
save(model_rf_hosp, model_rf_icu, model_rf_vent,model_rf_vent2,model_rf_mort,model_rf_mort2,file = "BestModelsclassification.RData")

```

