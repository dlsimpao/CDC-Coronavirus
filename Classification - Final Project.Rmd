---
title: "Untitled"
output: html_document
---

# Laoding Libraries
```{r}
library(tidyverse)
library(timeDate)
library(ggplot2)
library(plotly)
library(hrbrthemes)
#install.packages("hrbrthemes")
#install.packages("tseries")
library(tseries)
#install.packages("ggmap")
library(ggmap)
library(mltools)
library(data.table)
library(caret)
#install.packages("faux")
library(faux)
#install.packages("DataExplorer")
library(DataExplorer)
#install.packages('e1071', dependencies=TRUE)
library(e1071)
#install.packages("pRoc")
library(pROC)
library(ROSE)
#install.packages("doParallel")
#install.packages("broom")
library(broom)

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loading the data
```{r}
cdc_classification_new = read.csv('cdc_filtered_encoded.csv')
cdc_classification_new = cdc_classification_new %>% filter(sex != "Missing")
cdc_classification_new = cdc_classification_new[,2:32]
cdc_classification_new[,c(1,2,4,7:31)] = lapply(cdc_classification_new[,c(1,2,4,7:31)],factor) 

```

# Splitting data into training and test set

```{r}
set.seed(100)

# Step 1: Get row numbers for the training data
subset_classification <- cdc_classification_new %>% sample_frac(0.10)

trainRowNumbers <- createDataPartition(subset_classification$hosp_yn, p=0.75, list=FALSE)

# Step 2: Create the training  dataset
trainData <- subset_classification[trainRowNumbers,]

# Step 3: Create the test dataset
testData <- subset_classification[-trainRowNumbers,]




```

# Setting grid parameters for model fitting

```{r}
fitControl <- trainControl(
    method = 'cv',                   # k-fold cross validation
    number = 5,                      # number of folds
    savePredictions = 'final',       # saves predictions for optimal tuning parameter
    classProbs = TRUE,                  # should class probabilities be returned
    summaryFunction=twoClassSummary  # results summary function
) 
```


## Hospitalisation

# One hot Encoding
```{r}

train_hosp <- trainData %>% select(c("current_status", "sex", "hosp_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

test_hosp <- testData %>% select(c("current_status", "sex", "hosp_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

#head(train_hosp)



# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_hosp <- dummyVars(hosp_yn ~ . , data=train_hosp)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_hosp <- predict(dummies_model_hosp, newdata = train_hosp)

# # Convert to dataframe
trainData_hosp <- data.frame(trainData_hosp)

trainData_hosp$Hosp <- train_hosp$hosp_yn

# # See the structure of the new dataset
str(trainData_hosp)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_hosp_test <- dummyVars(hosp_yn ~ . , data=test_hosp)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
testData_hosp <- predict(dummies_model_hosp_test, newdata = test_hosp)

# # Convert to dataframe
testData_hosp <- data.frame(testData_hosp)

testData_hosp$Hosp <- test_hosp$hosp_yn

# # See the structure of the new dataset
str(testData_hosp)


```

# ROSE implementation for Oversampling

```{r}
data.rose_hosp <- ROSE(Hosp ~ ., data = trainData_hosp, seed = 1)$data
table(data.rose_hosp$Hosp) # balanced 0 = 8245, 1 = 8157
```


# Logistic Regression

```{r,warning=FALSE}
library(glmnet)
# Set the seed for reproducibility
set.seed(100)


my_glmnet <- getModelInfo("glmnet") %>% magrittr::extract2("glmnet")
my_glmnet$grid <- function (x, y, len = NULL, search = "grid") {
  if (search == "grid") {
    numLev <- if (is.character(y) | is.factor(y)) 
      length(levels(y))
    else NA
    if (!is.na(numLev)) {
      fam <- ifelse(numLev > 2, "multinomial", "binomial")
    }
    else fam <- "gaussian"
    init <- glmnet(as.matrix(x), y, family = fam, nlambda = 52, alpha = 0.5)
    lambda <- unique(init$lambda)
    lambda <- lambda[-c(1, length(lambda))]
    l_seq <- seq(1, length(lambda), length = len) %>% round %>% unique
    lambda <- lambda[l_seq]
    out <- expand.grid(alpha = seq(0.1, 1, length = len), 
                       lambda = lambda)
  }
  else {
    out <- data.frame(alpha = runif(len, min = 0, 1), lambda = 2^runif(len, 
                                                                       min = -10, 3))
  }
  out
}

levels(data.rose_hosp$Hosp) <- c("No","Yes")

# Training the model
model_logistic_hosp = caret::train(Hosp ~ ., data=data.rose_hosp, method= my_glmnet,trControl = fitControl)

#model_logistic_hosp

#plot(model_logistic, main="Model Accuracies with LR")

varimp_lr_hosp <- varImp(model_logistic_hosp)

varimp_lr_hosp

#plot(varimp_lr, main="Variable Importance with LR")

levels(testData_hosp$Hosp) <- c("No", "Yes")


#predicting for test set
predicted = predict(model_logistic_hosp,testData_hosp[,c(1:47)])

# Performance of the test set
perf_lr_hosp = confusionMatrix(reference = testData_hosp$Hosp, data = predicted, mode='everything')

perf_lr_hosp


# Roc plot
roc_hosp_lr = roc(as.integer(testData_hosp$Hosp),as.integer(predicted))
#plot(roc_hosp_lr)

# AUC
auc_hosp_lr = auc(roc_hosp_lr)
auc_hosp_lr

# Coefficients
coef(model_logistic_hosp$finalModel,model_logistic_hosp$bestTune$lambda)



```


#### SVM (takes time to run)

```{r}

# Training the model
levels(data.rose_hosp$Hosp) <- c("No", "Yes")

model_svm_hosp = caret::train(Hosp ~ .,
    data = data.rose_hosp,
    method = "svmLinear",
    preProc = NULL,
    trControl = fitControl)

model_svm_hosp



# Variable importance plot
varimp_svm_hosp <- varImp(model_svm_hosp)

varimp_svm_hosp

#plot(varimp_svm, main="Variable Importance with SVM")

levels(testData_hosp$Hosp) <- c("No", "Yes")

# Predicting on test set
predicted = predict(model_svm_hosp,testData_hosp[,c(1:47)])

# Performance
perf_svm_hosp = confusionMatrix(reference = testData_hosp$Hosp, data = predicted, mode='everything')

perf_svm_hosp

#ROC Curve
roc_hosp_svm = roc(as.integer(testData_hosp$Hosp),as.integer(predicted))
#plot(roc_hosp_svm)

#AUC
auc_hosp_svm = auc(roc_hosp_svm)
auc_hosp_svm


# Coefficients
coefs_hosp <- model_svm_hosp$finalModel@coef[[1]]
mat_hosp <- model_svm_hosp$finalModel@xmatrix[[1]]

w_hosp <- coefs_hosp %*% mat_hosp

w_hosp

```


#### Random Forest (Takes quite a bit of time to run)


```{r,warning=FALSE}

# Training the model
levels(data.rose_hosp$Hosp) <- c("No", "Yes")
model_rf_hosp = caret::train(Hosp ~ ., data=data.rose_hosp, method='rf', tuneLength= 10, trControl = fitControl,allowParallel = TRUE)
#model_rf

# Variable importance

varimp_rf_hosp <- varImp(model_rf_hosp)

varimp_rf_hosp


#plot(varimp_rf, main="Variable Importance with Random Forest")

levels(testData_hosp$Hosp) <- c("No", "Yes")

# Predicting test data
predicted = predict(model_rf_hosp,testData_hosp[,c(1:47)])

# Performance of test set
perf_rf_hosp = confusionMatrix(reference = testData_hosp$Hosp, data = predicted, mode='everything')

perf_rf_hosp


# ROC Curve
roc_hosp_rf = roc(as.integer(testData_hosp$Hosp),as.integer(predicted))
plot(roc_hosp_rf)

#AUC
auc_hosp_rf = auc(roc_hosp_rf)
auc_hosp_rf

```

# Saving the models
```{r}
save(model_logistic_hosp,model_svm_hosp,model_rf_hosp,varimp_lr_hosp,varimp_svm_hosp,varimp_rf_hosp,perf_lr_hosp,perf_svm_hosp,perf_rf_hosp,file = "HospModels_test.RData")
```


# ICU Need

```{r}
set.seed(100)

# Step 1: Get row numbers for the training data
subset_classification <- cdc_classification_new %>% sample_frac(0.10)

trainRowNumbers <- createDataPartition(subset_classification$icu_yn, p=0.75, list=FALSE)

# Step 2: Create the training  dataset
trainData <- subset_classification[trainRowNumbers,]

# Step 3: Create the test dataset
testData <- subset_classification[-trainRowNumbers,]




```

# One hot encoding

```{r}

train_icu <- trainData %>% select(c("current_status", "sex", "icu_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

test_icu <- testData %>% select(c("current_status", "sex", "icu_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

#head(train_icu)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_icu <- dummyVars(icu_yn ~ . , data=train_icu)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_icu <- predict(dummies_model_icu, newdata = train_icu)

# # Convert to dataframe
trainData_icu <- data.frame(trainData_icu)

trainData_icu$ICU <- train_icu$icu_yn

# # See the structure of the new dataset
str(trainData_icu)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_icu_test <- dummyVars(icu_yn ~ . , data=test_icu)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
testData_icu <- predict(dummies_model_icu_test, newdata = test_icu)

# # Convert to dataframe
testData_icu <- data.frame(testData_icu)

testData_icu$ICU <- test_icu$icu_yn

# # See the structure of the new dataset
str(testData_icu)


```

# ROSE implementation for oversampling to deal with unbalanced data

```{r}
data.rose_icu <- ROSE(ICU ~ ., data = trainData_icu, seed = 1)$data
table(data.rose_icu$ICU) # balanced 0 = 8245, 1 = 8157
```


# Logistic Regression

```{r}
library(glmnet)
# Set the seed for reproducibility
set.seed(100)


my_glmnet <- getModelInfo("glmnet") %>% magrittr::extract2("glmnet")
my_glmnet$grid <- function (x, y, len = NULL, search = "grid") {
  if (search == "grid") {
    numLev <- if (is.character(y) | is.factor(y)) 
      length(levels(y))
    else NA
    if (!is.na(numLev)) {
      fam <- ifelse(numLev > 2, "multinomial", "binomial")
    }
    else fam <- "gaussian"
    init <- glmnet(as.matrix(x), y, family = fam, nlambda = 52, alpha = 0.5)
    lambda <- unique(init$lambda)
    lambda <- lambda[-c(1, length(lambda))]
    l_seq <- seq(1, length(lambda), length = len) %>% round %>% unique
    lambda <- lambda[l_seq]
    out <- expand.grid(alpha = seq(0.1, 1, length = len), 
                       lambda = lambda)
  }
  else {
    out <- data.frame(alpha = runif(len, min = 0, 1), lambda = 2^runif(len, 
                                                                       min = -10, 3))
  }
  out
}

levels(data.rose_icu$ICU) <- c("No","Yes")

model_logistic_icu = caret::train(ICU ~ ., data=data.rose_icu, method= my_glmnet,trControl = fitControl)

#plot(model_logistic, main="Model Accuracies with LR")

varimp_lr_icu <- varImp(model_logistic_icu)

varimp_lr_icu

#plot(varimp_lr, main="Variable Importance with LR")

levels(testData_icu$ICU) <- c("No", "Yes")

predicted = predict(model_logistic_icu,testData_icu[,c(1:47)])

perf_lr_icu = confusionMatrix(reference = testData_icu$ICU, data = predicted, mode='everything')

perf_lr_icu


roc_icu_lr = roc(as.integer(testData_icu$ICU),as.integer(predicted))
#plot(roc_hosp_lr)

auc_icu_lr = auc(roc_icu_lr)
auc_icu_lr





coef(model_logistic_icu$finalModel,model_logistic_icu$bestTune$lambda)


```


#### SVM 

```{r}
set.seed(100)

# Training the model
levels(data.rose_icu$ICU) <- c("No", "Yes")

model_svm_icu =caret::train(ICU ~ .,
    data = data.rose_icu,
    method = "svmLinear",
    preProc = NULL,
    trControl = fitControl)


varimp_svm_icu <- varImp(model_svm_icu)

varimp_svm_icu

#plot(varimp_svm, main="Variable Importance with SVM")

levels(testData_icu$ICU) <- c("No", "Yes")

predicted = predict(model_svm_icu,testData_icu[,c(1:47)])

perf_svm_icu = confusionMatrix(reference = testData_icu$ICU, data = predicted, mode='everything')

perf_svm_icu


roc_icu_svm = roc(as.integer(testData_icu$ICU),as.integer(predicted))
#plot(roc_hosp_lr)

auc_icu_svm = auc(roc_icu_svm)
auc_icu_svm

coefs_icu <- model_svm_icu$finalModel@coef[[1]]
mat_icu <- model_svm_icu$finalModel@xmatrix[[1]]

w_icu <- coefs_icu %*% mat_icu

w_icu

```


#### Random Forest


```{r}
# Training the model 
levels(data.rose_icu$ICU) <- c("No", "Yes")
model_rf_icu = caret::train(ICU ~ ., data=data.rose_icu, method='rf', tuneLength= 10, trControl = fitControl,
                            allowParallel = T)
#model_rf

varimp_rf_icu <- varImp(model_rf_icu)

varimp_rf_icu

#plot(varimp_rf, main="Variable Importance with Random Forest")

levels(testData_icu$ICU) <- c("No", "Yes")

predicted = predict(model_rf_icu,testData_icu[,c(1:47)])

perf_rf_icu = confusionMatrix(reference = testData_icu$ICU, data = predicted, mode='everything')

perf_rf_icu

roc_icu_rf = roc(as.integer(testData_icu$ICU),as.integer(predicted))

plot(roc_icu_rf)

auc_icu_rf = auc(roc_icu_rf)
auc_icu_rf

```


```{r}
save(model_logistic_icu,model_svm_icu,model_rf_icu,varimp_lr_icu,varimp_svm_icu,varimp_rf_icu,perf_lr_icu,perf_svm_icu,perf_rf_icu,file = "ICUModels_test.RData")

```



### Ventilator Need (With only base features - First Model)

```{r}
set.seed(100)

# Step 1: Get row numbers for the training data
subset_classification <- cdc_classification_new %>% sample_frac(0.10)

trainRowNumbers <- createDataPartition(subset_classification$mechvent_yn, p=0.75, list=FALSE)

# Step 2: Create the training  dataset
trainData <- subset_classification[trainRowNumbers,]

# Step 3: Create the test dataset
testData <- subset_classification[-trainRowNumbers,]




```


# One hot encoding
```{r}

train_vent <- trainData %>% select(c("current_status", "sex", "mechvent_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

test_vent <- testData %>% select(c("current_status", "sex", "mechvent_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

#head(train_vent)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_vent <- dummyVars(mechvent_yn ~ . , data=train_vent)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_vent <- predict(dummies_model_vent, newdata = train_vent)

# # Convert to dataframe
trainData_vent <- data.frame(trainData_vent)

trainData_vent$Ventilator <- train_vent$mechvent_yn

# # See the structure of the new dataset
str(trainData_vent)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_vent_test <- dummyVars(mechvent_yn ~ . , data=test_vent)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
testData_vent <- predict(dummies_model_vent_test, newdata = test_vent)

# # Convert to dataframe
testData_vent <- data.frame(testData_vent)

testData_vent$Ventilator <- test_vent$mechvent_yn

# # See the structure of the new dataset
str(testData_vent)


```

# ROSE implementation

```{r}
data.rose_vent <- ROSE(Ventilator ~ ., data = trainData_vent, seed = 1)$data
table(data.rose_vent$Ventilator) # balanced 0 = 8245, 1 = 8157
```

### Logistic Regression

```{r}
library(glmnet)
# Set the seed for reproducibility
set.seed(100)


my_glmnet <- getModelInfo("glmnet") %>% magrittr::extract2("glmnet")
my_glmnet$grid <- function (x, y, len = NULL, search = "grid") {
  if (search == "grid") {
    numLev <- if (is.character(y) | is.factor(y)) 
      length(levels(y))
    else NA
    if (!is.na(numLev)) {
      fam <- ifelse(numLev > 2, "multinomial", "binomial")
    }
    else fam <- "gaussian"
    init <- glmnet(as.matrix(x), y, family = fam, nlambda = 52, alpha = 0.5)
    lambda <- unique(init$lambda)
    lambda <- lambda[-c(1, length(lambda))]
    l_seq <- seq(1, length(lambda), length = len) %>% round %>% unique
    lambda <- lambda[l_seq]
    out <- expand.grid(alpha = seq(0.1, 1, length = len), 
                       lambda = lambda)
  }
  else {
    out <- data.frame(alpha = runif(len, min = 0, 1), lambda = 2^runif(len, 
                                                                       min = -10, 3))
  }
  out
}


levels(data.rose_vent$Ventilator) <- c("No","Yes")

# Train the model
model_logistic_vent = caret::train(Ventilator ~ ., data=data.rose_vent, method= my_glmnet, trControl = fitControl )

#plot(model_logistic_vent, main="Model Accuracies with LR")

varimp_lr_vent <- varImp(model_logistic_vent)

varimp_lr_vent

#plot(varimp_lr, main="Variable Importance with LR")

levels(testData_vent$Ventilator) <- c("No","Yes")

predicted = predict(model_logistic_vent,testData_vent[,c(1:47)])

perf_lr_vent = confusionMatrix(reference = testData_vent$Ventilator, data = predicted, mode='everything')

perf_lr_vent


roc_vent_lr = roc(as.integer(testData_vent$Ventilator),as.integer(predicted))
#plot(roc_hosp_lr)

auc_vent_lr = auc(roc_vent_lr)
auc_vent_lr

coef(model_logistic_vent$finalModel,model_logistic_vent$bestTune$lambda)


```


#### SVM 

```{r}

set.seed(100)

# Training the model 
levels(data.rose_vent$Ventilator) <- c("No", "Yes")
model_svm_vent = caret::train(Ventilator ~ .,
    data = data.rose_vent,
    method = "svmLinear",
    preProc = NULL,
    trControl = fitControl)

#model_svmRadial

# Variable Importance

varimp_svm_vent <- varImp(model_svm_vent)

varimp_svm_vent

#plot(varimp_svm, main="Variable Importance with SVM")

levels(testData_vent$Ventilator) <- c("No", "Yes")

# Predicting on test data

predicted = predict(model_svm_vent,testData_vent[,c(1:47)])

perf_svm_vent = confusionMatrix(reference = testData_vent$Ventilator, data = predicted, mode='everything')

perf_svm_vent


roc_vent_svm = roc(as.integer(testData_vent$Ventilator),as.integer(predicted))
#plot(roc_hosp_lr)

auc_vent_svm = auc(roc_vent_svm)
auc_vent_svm


# Coefficients
coefs_vent <- model_svm_vent$finalModel@coef[[1]]
mat_vent <- model_svm_vent$finalModel@xmatrix[[1]]

w_vent <- coefs_vent %*% mat_vent

w_vent


```


#### Random Forest


```{r}
set.seed(100)

# Training the model
levels(data.rose_vent) <- c("No", "Yes")
model_rf_vent = caret::train(Ventilator ~ ., data=data.rose_vent, method='rf', tuneLength= 10, trControl = fitControl)
#model_rf

# Variable Importance

varimp_rf_vent <- varImp(model_rf_vent)

varimp_rf_vent

#plot(varimp_rf, main="Variable Importance with Random Forest")

levels(testData_vent$Ventilator) <- c("No", "Yes")

# Predicting Test data
predicted = predict(model_rf_vent,testData_vent[,c(1:47)])


# Performance
perf_rf_vent = confusionMatrix(reference = testData_vent$Ventilator, data = predicted, mode='everything')

perf_rf_vent

# ROC Curve
roc_vent_rf = roc(as.integer(testData_vent$Ventilator),as.integer(predicted))
#plot(roc_hosp_lr)

# AUC
auc_vent_rf = auc(roc_vent_rf)
auc_vent_rf

```

# Saving the models

```{r}
save(model_logistic_vent,model_svm_vent,model_rf_vent,varimp_lr_vent,varimp_svm_vent,varimp_rf_vent,perf_lr_vent,perf_svm_vent,perf_rf_vent,file = "Vent1Models_test.RData")

```


## Ventilator (with symptoms and ICU info - Second Model)

```{r}
#### ONE HOT ENCODING ######

train_vent2 <- trainData %>% select(c("current_status", "sex", "mechvent_yn", "hc_work_yn", "icu_yn", "pna_yn", "abxchest_yn" ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",    "cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

test_vent2 <- testData %>% select(c("current_status", "sex", "mechvent_yn", "hc_work_yn", "icu_yn", "pna_yn", "abxchest_yn" ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",   "cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

#head(train_vent2)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_vent2 <- dummyVars(mechvent_yn ~ . , data=train_vent2)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_vent2 <- predict(dummies_model_vent2, newdata = train_vent2)

# # Convert to dataframe
trainData_vent2 <- data.frame(trainData_vent2)

trainData_vent2$Ventilator <- train_vent2$mechvent_yn

# # See the structure of the new dataset
str(trainData_vent2)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_vent2_test <- dummyVars(mechvent_yn ~ . , data=test_vent2)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
testData_vent2 <- predict(dummies_model_vent2_test, newdata = test_vent2)

# # Convert to dataframe
testData_vent2 <- data.frame(testData_vent2)

testData_vent2$Ventilator <- test_vent2$mechvent_yn

# # See the structure of the new dataset
str(testData_vent2)


```

# ROSE implementation for oversampling

```{r}
data.rose_vent2 <- ROSE(Ventilator ~ ., data = trainData_vent2, seed = 1)$data
table(data.rose_vent2$Ventilator) # balanced 0 = 8245, 1 = 8157
```

### Logistic Regression

```{r}
library(glmnet)
# Set the seed for reproducibility
set.seed(100)


my_glmnet <- getModelInfo("glmnet") %>% magrittr::extract2("glmnet")
my_glmnet$grid <- function (x, y, len = NULL, search = "grid") {
  if (search == "grid") {
    numLev <- if (is.character(y) | is.factor(y)) 
      length(levels(y))
    else NA
    if (!is.na(numLev)) {
      fam <- ifelse(numLev > 2, "multinomial", "binomial")
    }
    else fam <- "gaussian"
    init <- glmnet(as.matrix(x), y, family = fam, nlambda = 52, alpha = 0.5)
    lambda <- unique(init$lambda)
    lambda <- lambda[-c(1, length(lambda))]
    l_seq <- seq(1, length(lambda), length = len) %>% round %>% unique
    lambda <- lambda[l_seq]
    out <- expand.grid(alpha = seq(0.1, 1, length = len), 
                       lambda = lambda)
  }
  else {
    out <- data.frame(alpha = runif(len, min = 0, 1), lambda = 2^runif(len, 
                                                                       min = -10, 3))
  }
  out
}


levels(data.rose_vent2$Ventilator) <- c("No","Yes")

# Train the model
model_logistic_vent2 = caret::train(Ventilator ~ ., data=data.rose_vent2, method= my_glmnet, trControl = fitControl )

#plot(model_logistic_vent, main="Model Accuracies with LR")

# Variable Importance
varimp_lr_vent2 <- varImp(model_logistic_vent2)

varimp_lr_vent2

#plot(varimp_lr, main="Variable Importance with LR")

levels(testData_vent2$Ventilator) <- c("No","Yes")

# Predictingg on Test data
predicted = predict(model_logistic_vent2,testData_vent2[,c(1:49)])

# Performance
perf_lr_vent2 = confusionMatrix(reference = testData_vent2$Ventilator, data = predicted, mode='everything')

perf_lr_vent2

# ROC Curve
roc_vent2_lr = roc(as.integer(testData_vent2$Ventilator),as.integer(predicted))
#plot(roc_hosp_lr)

# AUC
auc_vent2_lr = auc(roc_vent2_lr)
auc_vent2_lr

# Coefficients
coef(model_logistic_vent2$finalModel,model_logistic_vent2$bestTune$lambda)


```


#### SVM 

```{r}

set.seed(100)

# Training the model
levels(data.rose_vent2$Ventilator) <- c("No", "Yes")
model_svm_vent2 = caret::train(Ventilator ~ .,
    data = data.rose_vent2,
    method = "svmLinear",
    preProc = NULL,
    trControl = fitControl)

#model_svmRadial

# Variable Importance

varimp_svm_vent2 <- varImp(model_svm_vent2)

varimp_svm_vent2

#plot(varimp_svm, main="Variable Importance with SVM")

levels(testData_vent2$Ventilator) <- c("No", "Yes")

# Predicting on Test Data
predicted = predict(model_svm_vent2,testData_vent2[,c(1:49)])

# Performance of test data
perf_svm_vent2 = confusionMatrix(reference = testData_vent2$Ventilator, data = predicted, mode='everything')

perf_svm_vent2

# ROC Curve
roc_vent2_svm = roc(as.integer(testData_vent2$Ventilator),as.integer(predicted))
#plot(roc_hosp_lr)

# AUC
auc_vent2_svm = auc(roc_vent2_svm)
auc_vent2_svm

# Coefficients
coefs_vent2 <- model_svm_vent2$finalModel@coef[[1]]
mat_vent2 <- model_svm_vent2$finalModel@xmatrix[[1]]

w_vent2 <- coefs_vent2 %*% mat_vent2

w_vent2

```


#### Random Forest


```{r}
set.seed(100)

# Training the model
levels(data.rose_vent2$Ventilator) <- c("No", "Yes")
model_rf_vent2 = caret::train(Ventilator ~ ., data=data.rose_vent2, method='rf', tuneLength= 10, trControl = fitControl)
#model_rf

# Variable Importance

varimp_rf_vent2 <- varImp(model_rf_vent2)

varimp_rf_vent2

#plot(varimp_rf, main="Variable Importance with Random Forest")

levels(testData_vent2$Ventilator) <- c("No", "Yes")

# Predicting on test data
predicted = predict(model_rf_vent2,testData_vent2[,c(1:49)])

# Performance
perf_rf_vent2 = confusionMatrix(reference = testData_vent2$Ventilator, data = predicted, mode='everything')

perf_rf_vent2

# ROC Curve
roc_vent2_rf = roc(as.integer(testData_vent2$Ventilator),as.integer(predicted))
#plot(roc_hosp_lr)

# AUC
auc_vent2_rf = auc(roc_vent2_rf)
auc_vent2_rf

```

# Save the models

```{r}

save(model_logistic_vent2,model_svm_vent2,model_rf_vent2,varimp_lr_vent2,varimp_svm_vent2,varimp_rf_vent2,perf_lr_vent2,perf_svm_vent2,perf_rf_vent2,file = "Vent2Models_test.RData")

```



# Mortality (With only base features - Model one)

```{r}
set.seed(100)

# Step 1: Get row numbers for the training data
subset_classification <- cdc_classification_new %>% sample_frac(0.10)

trainRowNumbers <- createDataPartition(subset_classification$death_yn, p=0.75, list=FALSE)

# Step 2: Create the training  dataset
trainData <- subset_classification[trainRowNumbers,]

# Step 3: Create the test dataset
testData <- subset_classification[-trainRowNumbers,]




```

# One hot encoding

```{r}

train_mort <- trainData %>% select(c("current_status", "sex", "death_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

test_mort <- testData %>% select(c("current_status", "sex", "death_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

#head(train_icu)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_mort <- dummyVars(death_yn ~ . , data=train_mort)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mort <- predict(dummies_model_mort, newdata = train_mort)

# # Convert to dataframe
trainData_mort <- data.frame(trainData_mort)

trainData_mort$Mortality <- train_mort$death_yn

# # See the structure of the new dataset
str(trainData_mort)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_mort_test <- dummyVars(death_yn ~ . , data=test_mort)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
testData_mort <- predict(dummies_model_mort_test, newdata = test_mort)

# # Convert to dataframe
testData_mort <- data.frame(testData_mort)

testData_mort$Mortality <- test_mort$death_yn

# # See the structure of the new dataset
str(testData_mort)


```

# ROSE implementation for oversampling

```{r}
data.rose_mort <- ROSE(Mortality ~ ., data = trainData_mort, seed = 1)$data
table(data.rose_mort$Mortality) # balanced 0 = 8245, 1 = 8157
```

# Logistic Reg

```{r}
library(glmnet)
# Set the seed for reproducibility
set.seed(100)


my_glmnet <- getModelInfo("glmnet") %>% magrittr::extract2("glmnet")
my_glmnet$grid <- function (x, y, len = NULL, search = "grid") {
  if (search == "grid") {
    numLev <- if (is.character(y) | is.factor(y)) 
      length(levels(y))
    else NA
    if (!is.na(numLev)) {
      fam <- ifelse(numLev > 2, "multinomial", "binomial")
    }
    else fam <- "gaussian"
    init <- glmnet(as.matrix(x), y, family = fam, nlambda = 52, alpha = 0.5)
    lambda <- unique(init$lambda)
    lambda <- lambda[-c(1, length(lambda))]
    l_seq <- seq(1, length(lambda), length = len) %>% round %>% unique
    lambda <- lambda[l_seq]
    out <- expand.grid(alpha = seq(0.1, 1, length = len), 
                       lambda = lambda)
  }
  else {
    out <- data.frame(alpha = runif(len, min = 0, 1), lambda = 2^runif(len, 
                                                                       min = -10, 3))
  }
  out
}


levels(data.rose_mort$Mortality) <- c("No", "Yes")

# Train the model
model_logistic_mort = caret::train(Mortality ~ ., data=data.rose_mort, method= my_glmnet,trControl = fitControl)

#plot(model_logistic, main="Model Accuracies with LR")


# Variable Importance
varimp_lr_mort <- varImp(model_logistic_mort)

varimp_lr_mort

#plot(varimp_lr, main="Variable Importance with LR")

levels(testData_mort$Mortality) <- c("No", "Yes")

# Predicting on Test data
predicted = predict(model_logistic_mort,testData_mort[,c(1:47)])

# Performance of test set
perf_lr_mort = confusionMatrix(reference = testData_mort$Mortality, data = predicted, mode='everything')

perf_lr_mort

# ROC Curve
roc_mort_lr = roc(as.integer(testData_mort$Mortality),as.integer(predicted))
#plot(roc_hosp_lr)

# AUC
auc_mort_lr = auc(roc_mort_lr)
auc_mort_lr

# Coefficients
coef(model_logistic_mort$finalModel,model_logistic_mort$bestTune$lambda)




```

#### SVM 

```{r}

# Training the model 
levels(data.rose_mort$Mortality) <- c("No", "Yes")
model_svm_mort = caret::train(Mortality ~ .,
    data = data.rose_mort,
    method = "svmLinear",
    preProc = NULL,
    trControl = fitControl)

# Variable Importance

varimp_svm_mort <- varImp(model_svm_mort)

varimp_svm_mort

#plot(varimp_svm, main="Variable Importance with SVM")

levels(testData_mort$Mortality) <- c("No", "Yes")

# Predicting on test set
predicted = predict(model_svm_mort,testData_mort[,c(1:47)])

# Performance
perf_svm_mort = confusionMatrix(reference = testData_mort$Mortality, data = predicted, mode='everything')

perf_svm_mort

# ROC Curve
roc_mort_svm = roc(as.integer(testData_mort$Mortality),as.integer(predicted))
#plot(roc_hosp_lr)

# AUC
auc_mort_svm = auc(roc_mort_svm)
auc_mort_svm

# Coefficients
coefs_mort <- model_svm_mort$finalModel@coef[[1]]
mat_mort <- model_svm_mort$finalModel@xmatrix[[1]]

w_mort <- coefs_mort %*% mat_mort

w_mort



```


#### Random Forest


```{r}
# Training the model
levels(data.rose_mort$Mortality) <- c("No", "Yes")
model_rf_mort = caret::train(Mortality ~ ., data=data.rose_mort, method='rf', tuneLength= 10, trControl = fitControl)
#model_rf

# Variable Importance

varimp_rf_mort <- varImp(model_rf_mort)

varimp_rf_mort

#plot(varimp_rf, main="Variable Importance with Random Forest")

levels(testData_mort$Mortality) <- c("No", "Yes")

# Predicting on test data
predicted = predict(model_rf_mort,testData_mort[,c(1:47)])

# Performance of test set
perf_rf_mort = confusionMatrix(reference = testData_mort$Mortality, data = predicted, mode='everything')

perf_rf_mort

# ROC Curve
roc_mort_rf = roc(as.integer(testData_mort$Mortality),as.integer(predicted))
#plot(roc_hosp_lr)

# AUC
auc_mort_rf = auc(roc_mort_rf)
auc_mort_rf

```

# Saving the models

```{r}

save(model_logistic_mort,model_svm_mort,model_rf_mort,varimp_lr_mort,varimp_svm_mort,varimp_rf_mort,perf_lr_mort,perf_svm_mort,perf_rf_mort,file = "Mort1Models_test.RData")

```


### Mortality (With base features, hospitalization, ICU and ventilator info - Second Model)


# One hot Encoding
```{r}

train_mort2 <- trainData %>% select(c("current_status", "sex","hosp_yn","icu_yn","mechvent_yn" ,"death_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn","cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

test_mort2 <- testData %>% select(c("current_status", "sex", "hosp_yn","icu_yn","mechvent_yn" , "death_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn", "cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

#head(train_mort2)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_mort2 <- dummyVars(death_yn ~ . , data=train_mort2)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mort2 <- predict(dummies_model_mort2, newdata = train_mort2)

# # Convert to dataframe
trainData_mort2 <- data.frame(trainData_mort2)

trainData_mort2$Mortality <- train_mort2$death_yn

# # See the structure of the new dataset
str(trainData_mort2)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_mort2_test <- dummyVars(death_yn ~ . , data=test_mort2)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
testData_mort2 <- predict(dummies_model_mort2_test, newdata = test_mort2)

# # Convert to dataframe
testData_mort2 <- data.frame(testData_mort2)

testData_mort2$Mortality <- test_mort2$death_yn

# # See the structure of the new dataset
str(testData_mort2)


```

# ROSE Implementation for oversampling minority class

```{r}
data.rose_mort2 <- ROSE(Mortality ~ ., data = trainData_mort2, seed = 1)$data
```


# Logistic Regression

```{r}
library(glmnet)
# Set the seed for reproducibility
set.seed(100)


my_glmnet <- getModelInfo("glmnet") %>% magrittr::extract2("glmnet")
my_glmnet$grid <- function (x, y, len = NULL, search = "grid") {
  if (search == "grid") {
    numLev <- if (is.character(y) | is.factor(y)) 
      length(levels(y))
    else NA
    if (!is.na(numLev)) {
      fam <- ifelse(numLev > 2, "multinomial", "binomial")
    }
    else fam <- "gaussian"
    init <- glmnet(as.matrix(x), y, family = fam, nlambda = 52, alpha = 0.5)
    lambda <- unique(init$lambda)
    lambda <- lambda[-c(1, length(lambda))]
    l_seq <- seq(1, length(lambda), length = len) %>% round %>% unique
    lambda <- lambda[l_seq]
    out <- expand.grid(alpha = seq(0.1, 1, length = len), 
                       lambda = lambda)
  }
  else {
    out <- data.frame(alpha = runif(len, min = 0, 1), lambda = 2^runif(len, 
                                                                       min = -10, 3))
  }
  out
}


levels(data.rose_mort2$Mortality) <- c("No", "Yes")

# Train the model
model_logistic_mort2 = caret::train(Mortality ~ ., data=data.rose_mort2, method= my_glmnet,trControl = fitControl)

#plot(model_logistic, main="Model Accuracies with LR")

# Variable Importance
varimp_lr_mort2 <- varImp(model_logistic_mort2)

varimp_lr_mort2

#plot(varimp_lr, main="Variable Importance with LR")

levels(testData_mort2$Mortality) <- c("No", "Yes")

# Predicitng on test data
predicted = predict(model_logistic_mort2,testData_mort2[,c(1:53)])

# Performance
perf_lr_mort2 = confusionMatrix(reference = testData_mort2$Mortality, data = predicted, mode='everything')

perf_lr_mort2

# ROC Curve
roc_mort2_lr = roc(as.integer(testData_mort2$Mortality),as.integer(predicted))
#plot(roc_hosp_lr)

# AUC
auc_mort2_lr = auc(roc_mort2_lr)
auc_mort2_lr

# Coefficients
coef(model_logistic_mort2$finalModel,model_logistic_mort2$bestTune$lambda)

```

#### SVM 

```{r}
set.seed(100)

# Training the model
levels(data.rose_mort2$Mortality) <- c("No", "Yes")
model_svm_mort2 =caret::train(Mortality ~ .,
    data = data.rose_mort2,
    method = "svmLinear",
    preProc = NULL,
    trControl = fitControl)
#model_svmRadial

# Variable Importance

varimp_svm_mort2 <- varImp(model_svm_mort2)

varimp_svm_mort2

#plot(varimp_svm, main="Variable Importance with SVM")

levels(testData_mort2$Mortality) <- c("No", "Yes")

# Predicting on Test data
predicted = predict(model_svm_mort2,testData_mort2[,c(1:53)])

# Performance
perf_svm_mort2 = confusionMatrix(reference = testData_mort2$Mortality, data = predicted, mode='everything')

perf_svm_mort2

# ROC Curve
roc_mort2_svm = roc(as.integer(testData_mort2$Mortality),as.integer(predicted))
#plot(roc_hosp_lr)

# AUC
auc_mort2_svm = auc(roc_mort2_svm)
auc_mort2_svm

# Coefficients
coefs_mort2 <- model_svm_mort2$finalModel@coef[[1]]
mat_mort2 <- model_svm_mort2$finalModel@xmatrix[[1]]

w_mort2 <- coefs_mort2 %*% mat_mort2

w_mort2

```


#### Random Forest


```{r}
# Training the model
levels(data.rose_mort2$Mortality) <- c("No", "Yes")
model_rf_mort2 = caret::train(Mortality ~ ., data=data.rose_mort2, method='rf', tuneLength= 10, trControl = fitControl)
#model_rf

# Variable Importance

varimp_rf_mort2 <- varImp(model_rf_mort2)

varimp_rf_mort2

#plot(varimp_rf, main="Variable Importance with Random Forest")

levels(testData_mort2$Mortality) <- c("No", "Yes")

# Predicting on test set
predicted = predict(model_rf_mort2,testData_mort2[,c(1:53)])

# Performance of test set
perf_rf_mort2 = confusionMatrix(reference = testData_mort2$Mortality, data = predicted, mode='everything')

perf_rf_mort2

# ROC Curve
roc_mort2_rf = roc(as.integer(testData_mort2$Mortality),as.integer(predicted))
#plot(roc_hosp_lr)

# AUC
auc_mort2_rf = auc(roc_mort2_rf)
auc_mort2_rf

```

# Saving the models
```{r}
save(model_logistic_mort2,model_svm_mort2,model_rf_mort2,varimp_lr_mort2,varimp_svm_mort2,varimp_rf_mort2,perf_lr_mort2,perf_svm_mort2,perf_rf_mort2,file = "Mort2Models_test.RData")

```



### Saving best models for each target variable

```{r}
save(model_logistic_hosp, model_logistic_icu, model_logistic_vent,model_svm_mort,model_svm_vent2,model_svm_mort2,file = "BestModels_test.RData")

```