
```{r}
library(tidyverse)
library(timeDate)
library(ggplot2)
library(plotly)
library(hrbrthemes)
#install.packages("hrbrthemes")
#install.packages("tseries")
library(tseries)
#install.packages("ggmap")
library(ggmap)
library(mltools)
library(data.table)
library(caret)
#install.packages("faux")
library(faux)
#install.packages("DataExplorer")
library(DataExplorer)
#install.packages('e1071', dependencies=TRUE)
library(e1071)

```

# Loading the data
```{r}
cdc_classification = read.csv('cdc_filtered_encoded.csv')
cdc_classification = cdc_classification %>% filter(sex != "Missing")
cdc_classification = cdc_classification[,2:32]
cdc_classification[,c(1,2,4,7:31)] = lapply(cdc_classification[,c(1,2,4,7:31)],factor) 

```



### Testing on subset

```{r}
set.seed(100)
subset_classification <- cdc_classification %>% sample_frac(0.05)

# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(subset_classification$hosp_yn, p=0.75, list=FALSE)

# Step 2: Create the training  dataset
trainData <- subset_classification[trainRowNumbers,]

# Step 3: Create the test dataset
testData <- subset_classification[-trainRowNumbers,]



```






```{r}

train_hosp <- trainData %>% select(c("current_status", "sex", "hosp_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

test_hosp <- testData %>% select(c("current_status", "sex", "hosp_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

#head(train_hosp)



# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_hosp <- dummyVars(hosp_yn ~ . , data=train_hosp)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_hosp <- predict(dummies_model_hosp, newdata = train_hosp)

# # Convert to dataframe
trainData_hosp <- data.frame(trainData_hosp)

trainData_hosp$Hosp <- train_hosp$hosp_yn

# # See the structure of the new dataset
str(trainData_hosp)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_hosp_test <- dummyVars(hosp_yn ~ . , data=test_hosp)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
testData_hosp <- predict(dummies_model_hosp_test, newdata = test_hosp)

# # Convert to dataframe
testData_hosp <- data.frame(testData_hosp)

testData_hosp$Hosp <- test_hosp$hosp_yn

# # See the structure of the new dataset
str(testData_hosp)


```




```{r}
library(glmnet)
# Set the seed for reproducibility
set.seed(100)


my_glmnet <- getModelInfo("glmnet") %>% magrittr::extract2("glmnet")
my_glmnet$grid <- function (x, y, len = NULL, search = "grid") {
  if (search == "grid") {
    numLev <- if (is.character(y) | is.factor(y)) 
      length(levels(y))
    else NA
    if (!is.na(numLev)) {
      fam <- ifelse(numLev > 2, "multinomial", "binomial")
    }
    else fam <- "gaussian"
    init <- glmnet(as.matrix(x), y, family = fam, nlambda = 52, alpha = 0.5)
    lambda <- unique(init$lambda)
    lambda <- lambda[-c(1, length(lambda))]
    l_seq <- seq(1, length(lambda), length = len) %>% round %>% unique
    lambda <- lambda[l_seq]
    out <- expand.grid(alpha = seq(0.1, 1, length = len), 
                       lambda = lambda)
  }
  else {
    out <- data.frame(alpha = runif(len, min = 0, 1), lambda = 2^runif(len, 
                                                                       min = -10, 3))
  }
  out
}


# Train the model using randomForest and predict on the training data itself.
model_logistic = train(Hosp ~ ., data=trainData_hosp, method= my_glmnet)

plot(model_logistic, main="Model Accuracies with LR")

varimp_lr <- varImp(model_logistic)

varimp_lr

#plot(varimp_lr, main="Variable Importance with LR")

predicted = predict(model_logistic,testData_hosp[,c(1:47)])

perf_lr = confusionMatrix(reference = testData_hosp$Hosp, data = predicted, mode='everything')

perf_lr


```

Accuracy : 0.9462          
                 95% CI : (0.9371, 0.9544)
    No Information Rate : 0.9198          
    P-Value [Acc > NIR] : 5.181e-08       
                                          
                  Kappa : 0.5543          
                                          
 Mcnemar's Test P-Value : 1.311e-12       
                                          
            Sensitivity : 0.9881          
            Specificity : 0.4658          
         Pos Pred Value : 0.9550          
         Neg Pred Value : 0.7727          
              Precision : 0.9550          
                 Recall : 0.9881          
                     F1 : 0.9712          
             Prevalence : 0.9198          
         Detection Rate : 0.9089          
   Detection Prevalence : 0.9517          
      Balanced Accuracy : 0.7269          
                                          
       'Positive' Class : 0  

```{r}
fitControl <- trainControl(
    method = 'cv',                   # k-fold cross validation
    number = 5,                      # number of folds
    savePredictions = 'final',       # saves predictions for optimal tuning parameter
    classProbs = T,                  # should class probabilities be returned
    summaryFunction=twoClassSummary  # results summary function
) 
```

```{r}
fitControl2 <- trainControl(
    method = 'cv',                   # k-fold cross validation
    number = 3,                      # number of folds
    savePredictions = 'final',       # saves predictions for optimal tuning parameter
    classProbs = T,                  # should class probabilities be returned
    summaryFunction=twoClassSummary  # results summary function
) 
```


#### SVM 

```{r}

# Training the model on training data
levels(trainData_hosp$Hosp) <- c("No", "Yes")
model_svmRadial = train(Hosp ~ ., data=trainData_hosp, method='svmRadial', tuneLength=15, trControl = fitControl)
#model_svmRadial

# Testing it on test data

varimp_svm <- varImp(model_svmRadial)

varimp_svm

#plot(varimp_svm, main="Variable Importance with SVM")

levels(testData_hosp$Hosp) <- c("No", "Yes")

predicted = predict(model_svmRadial,testData_hosp[,c(1:47)])

perf_svm = confusionMatrix(reference = testData_hosp$Hosp, data = predicted, mode='everything')

perf_svm



```
Accuracy : 0.9458         
                 95% CI : (0.9367, 0.954)
    No Information Rate : 0.9198         
    P-Value [Acc > NIR] : 7.987e-08      
                                         
                  Kappa : 0.5524         
                                         
 Mcnemar's Test P-Value : 2.809e-12      
                                         
            Sensitivity : 0.9877         
            Specificity : 0.4658         
         Pos Pred Value : 0.9550         
         Neg Pred Value : 0.7669         
              Precision : 0.9550         
                 Recall : 0.9877         
                     F1 : 0.9710         
             Prevalence : 0.9198         
         Detection Rate : 0.9085         
   Detection Prevalence : 0.9513         
      Balanced Accuracy : 0.7267         
                                         
       'Positive' Class : No 

#### Random Forest


```{r}
# Training the model on training data
levels(trainData_hosp$Hosp) <- c("No", "Yes")
model_rf = train(Hosp ~ ., data=trainData_hosp, method='rf', tuneLength= 10, trControl = fitControl)
#model_rf

# Testing it on test data

varimp_rf <- varImp(model_rf)

varimp_rf

#plot(varimp_rf, main="Variable Importance with Random Forest")

levels(testData_hosp$Hosp) <- c("No", "Yes")

predicted = predict(model_rf,testData_hosp[,c(1:47)])

perf_rf = confusionMatrix(reference = testData_hosp$Hosp, data = predicted, mode='everything')

perf_rf

```
        No  2486  124
       Yes   27   95
                                         
               Accuracy : 0.9447         
                 95% CI : (0.9355, 0.953)
    No Information Rate : 0.9198         
    P-Value [Acc > NIR] : 2.808e-07      
                                         
                  Kappa : 0.5302         
                                         
 Mcnemar's Test P-Value : 5.612e-15      
                                         
            Sensitivity : 0.9893         
            Specificity : 0.4338         
         Pos Pred Value : 0.9525         
         Neg Pred Value : 0.7787         
              Precision : 0.9525         
                 Recall : 0.9893         
                     F1 : 0.9705         
             Prevalence : 0.9198         
         Detection Rate : 0.9100         
   Detection Prevalence : 0.9553         
      Balanced Accuracy : 0.7115         
                                         
       'Positive' Class : No     

### XG Boost Dart 

```{r}
# Training the model on training data
levels(trainData_hosp$Hosp) <- c("No", "Yes")
model_xgbdart = train(Hosp ~ ., data=trainData_hosp, method='xgbDART', tuneLength= 3, trControl = fitControl2)
#model_xgbdart

# Testing it on test data

varimp_xgbdart <- varImp(model_xgbdart)

varimp_xgbdart

#plot(varimp_xgbdart, main="Variable Importance with XG Boost")

levels(testData_hosp$Hosp) <- c("No", "Yes")

predicted = predict(model_xgbdart,testData_hosp[,c(1:47)])

perf_xgbdart = confusionMatrix(reference = testData_hosp$Hosp, data = predicted, mode='everything')

#cv = 3, sample = 0.05,  
perf_xgbdart

# abxchest_yn.0	100.000000		
# pna_yn.0	97.153723		
# medcond_yn.0	23.999002		
# sob_yn.0	20.768095		
# acuterespdistress_yn.0	18.395689		
# age_group.80..Years	16.861363		
# fever_yn.0	15.769595		
# age_group.70...79.Years	12.923882		
# headache_yn.1	6.320604		
# age_group.50...59.Years	6.245333	
# age_group.20...29.Years	6.105308		
# runnose_yn.0	5.901823		
# headache_yn.0	5.044246		
# hc_work_yn.0	4.848716		
# pna_yn.1	4.475765		
# medcond_yn.1	4.301723		
# age_group.30...39.Years	3.569835		
# age_group.60...69.Years	3.011651		
# age_group.10...19.Years	2.791275		
# current_status.Laboratory.confirmed.case	2.758514
```
    No  2482  116
       Yes   31  103
                                          
               Accuracy : 0.9462          
                 95% CI : (0.9371, 0.9544)
    No Information Rate : 0.9198          
    P-Value [Acc > NIR] : 5.181e-08       
                                          
                  Kappa : 0.5566          
                                          
 Mcnemar's Test P-Value : 4.262e-12       
                                          
            Sensitivity : 0.9877          
            Specificity : 0.4703          
         Pos Pred Value : 0.9554          
         Neg Pred Value : 0.7687          
              Precision : 0.9554          
                 Recall : 0.9877          
                     F1 : 0.9712          
             Prevalence : 0.9198          
         Detection Rate : 0.9085          
   Detection Prevalence : 0.9510          
      Balanced Accuracy : 0.7290          
                                          
       'Positive' Class : No              
                             


### FOR ICU

```{r}

train_icu <- trainData %>% select(c("current_status", "sex", "icu_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

test_icu <- testData %>% select(c("current_status", "sex", "icu_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

#head(train_icu)



# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_icu <- dummyVars(icu_yn ~ . , data=train_icu)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_icu <- predict(dummies_model_icu, newdata = train_icu)

# # Convert to dataframe
trainData_icu <- data.frame(trainData_icu)

trainData_icu$ICU <- train_icu$icu_yn

# # See the structure of the new dataset
str(trainData_icu)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_icu_test <- dummyVars(icu_yn ~ . , data=test_icu)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
testData_icu <- predict(dummies_model_icu_test, newdata = test_icu)

# # Convert to dataframe
testData_icu <- data.frame(testData_icu)

testData_icu$ICU <- test_icu$icu_yn

# # See the structure of the new dataset
str(testData_icu)


```




```{r}
library(glmnet)
# Set the seed for reproducibility
set.seed(100)


my_glmnet <- getModelInfo("glmnet") %>% magrittr::extract2("glmnet")
my_glmnet$grid <- function (x, y, len = NULL, search = "grid") {
  if (search == "grid") {
    numLev <- if (is.character(y) | is.factor(y)) 
      length(levels(y))
    else NA
    if (!is.na(numLev)) {
      fam <- ifelse(numLev > 2, "multinomial", "binomial")
    }
    else fam <- "gaussian"
    init <- glmnet(as.matrix(x), y, family = fam, nlambda = 52, alpha = 0.5)
    lambda <- unique(init$lambda)
    lambda <- lambda[-c(1, length(lambda))]
    l_seq <- seq(1, length(lambda), length = len) %>% round %>% unique
    lambda <- lambda[l_seq]
    out <- expand.grid(alpha = seq(0.1, 1, length = len), 
                       lambda = lambda)
  }
  else {
    out <- data.frame(alpha = runif(len, min = 0, 1), lambda = 2^runif(len, 
                                                                       min = -10, 3))
  }
  out
}


#levels(trainData_hosp$Hosp) <- c("Yes", "No")

# Train the model using randomForest and predict on the training data itself.
model_logistic = train(ICU ~ ., data=trainData_icu, method= my_glmnet)

plot(model_logistic, main="Model Accuracies with LR")

varimp_lr <- varImp(model_logistic)

varimp_lr

#plot(varimp_lr, main="Variable Importance with LR")

#levels(testData_hosp$Hosp) <- c("Yes", "No")

predicted = predict(model_logistic,testData_icu[,c(1:47)])

perf_lr = confusionMatrix(reference = testData_icu$ICU, data = predicted, mode='everything')

perf_lr


#roc_hosp_lr = roc(as.integer(testData_hosp$Hosp),as.integer(predicted))
#plot(roc_hosp_lr)

#auc_hosp_lr = auc(roc_hosp_lr)
#auc_hosp_lr

```

# FIT CONTROL

```{r}
fitControl <- trainControl(
    method = 'cv',                   # k-fold cross validation
    number = 5,                      # number of folds
    savePredictions = 'final',       # saves predictions for optimal tuning parameter
    classProbs = T,                  # should class probabilities be returned
    summaryFunction=twoClassSummary  # results summary function
) 
```

#### SVM 

```{r}

# Training the model on training data
levels(trainData_icu$ICU) <- c("No", "Yes")
model_svmRadial = train(ICU ~ ., data=trainData_icu, method='svmRadial', tuneLength=15, trControl = fitControl)
#model_svmRadial

# Testing it on test data

varimp_svm <- varImp(model_svmRadial)

varimp_svm

#plot(varimp_svm, main="Variable Importance with SVM")

levels(testData_hosp$Hosp) <- c("No", "Yes")

predicted = predict(model_svmRadial,testData_icu[,c(1:47)])

perf_svm = confusionMatrix(reference = testData_icu$ICU, data = predicted, mode='everything')

perf_svm



```


#### Random Forest


```{r}
# Training the model on training data
levels(trainData_icu$ICU) <- c("No", "Yes")
model_rf = train(ICU ~ ., data=trainData_icu, method='rf', tuneLength= 10, trControl = fitControl)
#model_rf

# Testing it on test data

varimp_rf <- varImp(model_rf)

varimp_rf

#plot(varimp_rf, main="Variable Importance with Random Forest")

levels(testData_icu$ICU) <- c("No", "Yes")

predicted = predict(model_rf,testData_icu[,c(1:47)])

perf_rf = confusionMatrix(reference = testData_icu$ICU, data = predicted, mode='everything')

perf_rf

```


### XG Boost Dart 

```{r}
# Training the model on training data
levels(trainData_icu$ICU) <- c("No", "Yes")
model_xgbdart = train(ICU ~ ., data=trainData_icu, method='xgbDART', tuneLength= 5, trControl = fitControl)
#model_xgbdart

# Testing it on test data

varimp_xgbdart <- varImp(model_xgbdart)

varimp_xgbdart

#plot(varimp_xgbdart, main="Variable Importance with XG Boost")

levels(testData_icu$ICU) <- c("No", "Yes")

predicted = predict(model_xgbdart,testData_icu[,c(1:47)])

perf_xgbdart = confusionMatrix(reference = testData_icu$ICU, data = predicted, mode='everything')

perf_xgbdart

```

#### Ventilator (with symptoms only)

```{r}

train_vent <- trainData %>% select(c("current_status", "sex", "mechvent_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

test_vent <- testData %>% select(c("current_status", "sex", "mechvent_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

#head(train_vent)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_vent <- dummyVars(mechvent_yn ~ . , data=train_vent)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_vent <- predict(dummies_model_vent, newdata = train_vent)

# # Convert to dataframe
trainData_vent <- data.frame(trainData_vent)

trainData_vent$Ventilator <- train_vent$mechvent_yn

# # See the structure of the new dataset
str(trainData_vent)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_vent_test <- dummyVars(mechvent_yn ~ . , data=test_vent)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
testData_vent <- predict(dummies_model_vent_test, newdata = test_vent)

# # Convert to dataframe
testData_vent <- data.frame(testData_vent)

testData_vent$Ventilator <- test_vent$mechvent_yn

# # See the structure of the new dataset
str(testData_vent)


```




```{r}
library(glmnet)
# Set the seed for reproducibility
set.seed(100)


my_glmnet <- getModelInfo("glmnet") %>% magrittr::extract2("glmnet")
my_glmnet$grid <- function (x, y, len = NULL, search = "grid") {
  if (search == "grid") {
    numLev <- if (is.character(y) | is.factor(y)) 
      length(levels(y))
    else NA
    if (!is.na(numLev)) {
      fam <- ifelse(numLev > 2, "multinomial", "binomial")
    }
    else fam <- "gaussian"
    init <- glmnet(as.matrix(x), y, family = fam, nlambda = 52, alpha = 0.5)
    lambda <- unique(init$lambda)
    lambda <- lambda[-c(1, length(lambda))]
    l_seq <- seq(1, length(lambda), length = len) %>% round %>% unique
    lambda <- lambda[l_seq]
    out <- expand.grid(alpha = seq(0.1, 1, length = len), 
                       lambda = lambda)
  }
  else {
    out <- data.frame(alpha = runif(len, min = 0, 1), lambda = 2^runif(len, 
                                                                       min = -10, 3))
  }
  out
}


#levels(trainData_hosp$Hosp) <- c("Yes", "No")

# Train the model using randomForest and predict on the training data itself.
model_logistic = train(Ventilator ~ ., data=trainData_vent, method= my_glmnet)

plot(model_logistic, main="Model Accuracies with LR")

varimp_lr <- varImp(model_logistic)

varimp_lr

#plot(varimp_lr, main="Variable Importance with LR")

#levels(testData_hosp$Hosp) <- c("Yes", "No")

predicted = predict(model_logistic,testData_vent[,c(1:47)])

perf_lr = confusionMatrix(reference = testData_vent$Ventilator, data = predicted, mode='everything')

perf_lr


#roc_hosp_lr = roc(as.integer(testData_hosp$Hosp),as.integer(predicted))
#plot(roc_hosp_lr)

#auc_hosp_lr = auc(roc_hosp_lr)
#auc_hosp_lr

```

# FIT CONTROL

```{r}
fitControl <- trainControl(
    method = 'cv',                   # k-fold cross validation
    number = 5,                      # number of folds
    savePredictions = 'final',       # saves predictions for optimal tuning parameter
    classProbs = T,                  # should class probabilities be returned
    summaryFunction=twoClassSummary  # results summary function
) 
```

#### SVM 

```{r}

# Training the model on training data
levels(trainData_vent$Ventilator) <- c("No", "Yes")
model_svmRadial = train(Ventilator ~ ., data=trainData_vent, method='svmRadial', tuneLength=15, trControl = fitControl)
#model_svmRadial

# Testing it on test data

varimp_svm <- varImp(model_svmRadial)

varimp_svm

#plot(varimp_svm, main="Variable Importance with SVM")

levels(testData_vent$Ventilator) <- c("No", "Yes")

predicted = predict(model_svmRadial,testData_vent[,c(1:47)])

perf_svm = confusionMatrix(reference = testData_vent$Ventilator, data = predicted, mode='everything')

perf_svm



```


#### Random Forest


```{r}
# Training the model on training data
levels(trainData_vent$Ventilator) <- c("No", "Yes")
model_rf = train(Ventilator ~ ., data=trainData_vent, method='rf', tuneLength= 10, trControl = fitControl)
#model_rf

# Testing it on test data

varimp_rf <- varImp(model_rf)

varimp_rf

#plot(varimp_rf, main="Variable Importance with Random Forest")

levels(testData_vent$Ventilator) <- c("No", "Yes")

predicted = predict(model_rf,testData_vent[,c(1:47)])

perf_rf = confusionMatrix(reference = testData_vent$Ventilator, data = predicted, mode='everything')

perf_rf

```


### XG Boost Dart 

```{r}
# Training the model on training data
levels(trainData_vent$Ventilator) <- c("No", "Yes")
model_xgbdart = train(Ventilator ~ ., data=trainData_vent, method='xgbDART', tuneLength= 5, trControl = fitControl)
#model_xgbdart

# Testing it on test data

varimp_xgbdart <- varImp(model_xgbdart)


varimp_xgbdart

#plot(varimp_xgbdart, main="Variable Importance with XG Boost")

levels(testData_vent$Ventilator) <- c("No", "Yes")

predicted = predict(model_xgbdart,testData_vent[,c(1:47)])

perf_xgbdart = confusionMatrix(reference = testData_vent$Ventilator, data = predicted, mode='everything')

perf_xgbdart


```

## Ventilator (with symptoms and ICU info)

```{r}

train_vent2 <- trainData %>% select(c("current_status", "sex", "mechvent_yn", "hc_work_yn", "icu_yn", "pna_yn", "abxchest_yn" ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",    "cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

test_vent2 <- testData %>% select(c("current_status", "sex", "mechvent_yn", "hc_work_yn", "icu_yn", "pna_yn", "abxchest_yn" ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",   "cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

#head(train_vent2)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_vent2 <- dummyVars(mechvent_yn ~ . , data=train_vent2)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_vent2 <- predict(dummies_model_vent2, newdata = train_vent2)

# # Convert to dataframe
trainData_vent2 <- data.frame(trainData_vent2)

trainData_vent2$Ventilator <- train_vent2$mechvent_yn

# # See the structure of the new dataset
str(trainData_vent2)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_vent2_test <- dummyVars(mechvent_yn ~ . , data=test_vent2)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
testData_vent2 <- predict(dummies_model_vent2_test, newdata = test_vent2)

# # Convert to dataframe
testData_vent2 <- data.frame(testData_vent2)

testData_vent2$Ventilator <- test_vent2$mechvent_yn

# # See the structure of the new dataset
str(testData_vent2)


```


# Logistic Reg

```{r}
library(glmnet)
# Set the seed for reproducibility
set.seed(100)


my_glmnet <- getModelInfo("glmnet") %>% magrittr::extract2("glmnet")
my_glmnet$grid <- function (x, y, len = NULL, search = "grid") {
  if (search == "grid") {
    numLev <- if (is.character(y) | is.factor(y)) 
      length(levels(y))
    else NA
    if (!is.na(numLev)) {
      fam <- ifelse(numLev > 2, "multinomial", "binomial")
    }
    else fam <- "gaussian"
    init <- glmnet(as.matrix(x), y, family = fam, nlambda = 52, alpha = 0.5)
    lambda <- unique(init$lambda)
    lambda <- lambda[-c(1, length(lambda))]
    l_seq <- seq(1, length(lambda), length = len) %>% round %>% unique
    lambda <- lambda[l_seq]
    out <- expand.grid(alpha = seq(0.1, 1, length = len), 
                       lambda = lambda)
  }
  else {
    out <- data.frame(alpha = runif(len, min = 0, 1), lambda = 2^runif(len, 
                                                                       min = -10, 3))
  }
  out
}


#levels(trainData_hosp$Hosp) <- c("Yes", "No")

# Train the model using randomForest and predict on the training data itself.
model_logistic = train(Ventilator ~ ., data=trainData_vent2, method= my_glmnet)

plot(model_logistic, main="Model Accuracies with LR")

varimp_lr <- varImp(model_logistic)

varimp_lr

#plot(varimp_lr, main="Variable Importance with LR")

#levels(testData_hosp$Hosp) <- c("Yes", "No")

predicted = predict(model_logistic,testData_vent2[,c(1:48)])

perf_lr = confusionMatrix(reference = testData_vent2$Ventilator, data = predicted, mode='everything')

perf_lr


#roc_hosp_lr = roc(as.integer(testData_hosp$Hosp),as.integer(predicted))
#plot(roc_hosp_lr)

#auc_hosp_lr = auc(roc_hosp_lr)
#auc_hosp_lr

```

# FIT CONTROL

```{r}
fitControl <- trainControl(
    method = 'cv',                   # k-fold cross validation
    number = 5,                      # number of folds
    savePredictions = 'final',       # saves predictions for optimal tuning parameter
    classProbs = T,                  # should class probabilities be returned
    summaryFunction=twoClassSummary  # results summary function
) 
```

#### SVM 

```{r}

# Training the model on training data
levels(trainData_vent2$Ventilator) <- c("No", "Yes")
model_svmRadial = train(Ventilator ~ ., data=trainData_vent2, method='svmRadial', tuneLength=15, trControl = fitControl)
#model_svmRadial

# Testing it on test data

varimp_svm <- varImp(model_svmRadial)

varimp_svm

#plot(varimp_svm, main="Variable Importance with SVM")

levels(testData_vent2$Ventilator) <- c("No", "Yes")

predicted = predict(model_svmRadial,testData_vent2[,c(1:48)])

perf_svm = confusionMatrix(reference = testData_vent2$Ventilator, data = predicted, mode='everything')

perf_svm



```


#### Random Forest


```{r}
# Training the model on training data
levels(trainData_vent2$Ventilator) <- c("No", "Yes")
model_rf = train(Ventilator ~ ., data=trainData_vent2, method='rf', tuneLength= 10, trControl = fitControl)
#model_rf

# Testing it on test data

varimp_rf <- varImp(model_rf)

varimp_rf

#plot(varimp_rf, main="Variable Importance with Random Forest")

levels(testData_vent2$Ventilator) <- c("No", "Yes")

predicted = predict(model_rf,testData_vent2[,c(1:48)])

perf_rf = confusionMatrix(reference = testData_vent2$Ventilator, data = predicted, mode='everything')

perf_rf

```


### XG Boost Dart 

```{r}
# Training the model on training data
levels(trainData_vent2$Ventilator) <- c("No", "Yes")
model_xgbdart = train(Ventilator ~ ., data=trainData_vent2, method='xgbDART', tuneLength= 5, trControl = fitControl)
#model_xgbdart

# Testing it on test data

varimp_xgbdart <- varImp(model_xgbdart)

varimp_xgbdart

#plot(varimp_xgbdart, main="Variable Importance with XG Boost")

levels(testData_vent2$Ventilator) <- c("No", "Yes")

predicted = predict(model_xgbdart,testData_vent2[,c(1:48)])

perf_xgbdart = confusionMatrix(reference = testData_vent2$Ventilator, data = predicted, mode='everything')

perf_xgbdart


```

### Mortality (With only symptoms)

```{r}

train_mort <- trainData %>% select(c("current_status", "sex", "death_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

test_mort <- testData %>% select(c("current_status", "sex", "death_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

#head(train_icu)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_mort <- dummyVars(death_yn ~ . , data=train_mort)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mort <- predict(dummies_model_mort, newdata = train_mort)

# # Convert to dataframe
trainData_mort <- data.frame(trainData_mort)

trainData_mort$Mortality <- train_mort$death_yn

# # See the structure of the new dataset
str(trainData_mort)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_mort_test <- dummyVars(death_yn ~ . , data=test_mort)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
testData_mort <- predict(dummies_model_mort_test, newdata = test_mort)

# # Convert to dataframe
testData_mort <- data.frame(testData_mort)

testData_mort$Mortality <- test_mort$death_yn

# # See the structure of the new dataset
str(testData_mort)


```



# Logistic Reg

```{r}
library(glmnet)
# Set the seed for reproducibility
set.seed(100)


my_glmnet <- getModelInfo("glmnet") %>% magrittr::extract2("glmnet")
my_glmnet$grid <- function (x, y, len = NULL, search = "grid") {
  if (search == "grid") {
    numLev <- if (is.character(y) | is.factor(y)) 
      length(levels(y))
    else NA
    if (!is.na(numLev)) {
      fam <- ifelse(numLev > 2, "multinomial", "binomial")
    }
    else fam <- "gaussian"
    init <- glmnet(as.matrix(x), y, family = fam, nlambda = 52, alpha = 0.5)
    lambda <- unique(init$lambda)
    lambda <- lambda[-c(1, length(lambda))]
    l_seq <- seq(1, length(lambda), length = len) %>% round %>% unique
    lambda <- lambda[l_seq]
    out <- expand.grid(alpha = seq(0.1, 1, length = len), 
                       lambda = lambda)
  }
  else {
    out <- data.frame(alpha = runif(len, min = 0, 1), lambda = 2^runif(len, 
                                                                       min = -10, 3))
  }
  out
}


#levels(trainData_hosp$Hosp) <- c("Yes", "No")

# Train the model using randomForest and predict on the training data itself.
model_logistic = train(Mortality ~ ., data=trainData_mort, method= my_glmnet)

plot(model_logistic, main="Model Accuracies with LR")

varimp_lr <- varImp(model_logistic)

varimp_lr

#plot(varimp_lr, main="Variable Importance with LR")

#levels(testData_hosp$Hosp) <- c("Yes", "No")

predicted = predict(model_logistic,testData_mort[,c(1:47)])

perf_lr = confusionMatrix(reference = testData_mort$Mortality, data = predicted, mode='everything')

perf_lr


#roc_hosp_lr = roc(as.integer(testData_hosp$Hosp),as.integer(predicted))
#plot(roc_hosp_lr)

#auc_hosp_lr = auc(roc_hosp_lr)
#auc_hosp_lr

```

# FIT CONTROL

```{r}
fitControl <- trainControl(
    method = 'cv',                   # k-fold cross validation
    number = 5,                      # number of folds
    savePredictions = 'final',       # saves predictions for optimal tuning parameter
    classProbs = T,                  # should class probabilities be returned
    summaryFunction=twoClassSummary  # results summary function
) 
```

#### SVM 

```{r}

# Training the model on training data
levels(trainData_mort$Mortality) <- c("No", "Yes")
model_svmRadial = train(Mortality ~ ., data=trainData_mort, method='svmRadial', tuneLength=15, trControl = fitControl)
#model_svmRadial

# Testing it on test data

varimp_svm <- varImp(model_svmRadial)

varimp_svm

#plot(varimp_svm, main="Variable Importance with SVM")

levels(testData_mort$Mortality) <- c("No", "Yes")

predicted = predict(model_svmRadial,testData_mort[,c(1:47)])

perf_svm = confusionMatrix(reference = testData_mort$Mortality, data = predicted, mode='everything')

perf_svm



```


#### Random Forest


```{r}
# Training the model on training data
levels(trainData_mort$Mortality) <- c("No", "Yes")
model_rf = train(Mortality ~ ., data=trainData_mort, method='rf', tuneLength= 10, trControl = fitControl)
#model_rf

# Testing it on test data

varimp_rf <- varImp(model_rf)

varimp_rf

#plot(varimp_rf, main="Variable Importance with Random Forest")

levels(testData_mort$Mortality) <- c("No", "Yes")

predicted = predict(model_rf,testData_mort[,c(1:47)])

perf_rf = confusionMatrix(reference = testData_mort$Mortality, data = predicted, mode='everything')

perf_rf

```


### XG Boost Dart 

```{r}
# Training the model on training data
levels(trainData_mort$Mortality) <- c("No", "Yes")
model_xgbdart = train(Mortality ~ ., data=trainData_mort, method='xgbDART', tuneLength= 5, trControl = fitControl)
#model_xgbdart

# Testing it on test data

varimp_xgbdart <- varImp(model_xgbdart)

varimp_xgbdart

#plot(varimp_xgbdart, main="Variable Importance with XG Boost")

levels(testData_mort$Mortality) <- c("No", "Yes")

predicted = predict(model_xgbdart,testData_mort[,c(1:47)])

perf_xgbdart = confusionMatrix(reference = testData_mort$Mortality, data = predicted, mode='everything')

perf_xgbdart


```

### Mortality (With symptoms, hospitalization, ICU and ventilator)

```{r}

train_mort2 <- trainData %>% select(c("current_status", "sex","hosp_yn","icu_yn","mechvent_yn" ,"death_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn","cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

test_mort2 <- testData %>% select(c("current_status", "sex", "hosp_yn","icu_yn","mechvent_yn" , "death_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn", "cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

#head(train_mort2)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_mort2 <- dummyVars(death_yn ~ . , data=train_mort2)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mort2 <- predict(dummies_model_mort2, newdata = train_mort2)

# # Convert to dataframe
trainData_mort2 <- data.frame(trainData_mort2)

trainData_mort2$Mortality <- train_mort2$death_yn

# # See the structure of the new dataset
str(trainData_mort2)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_mort2_test <- dummyVars(death_yn ~ . , data=test_mort2)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
testData_mort2 <- predict(dummies_model_mort2_test, newdata = test_mort2)

# # Convert to dataframe
testData_mort2 <- data.frame(testData_mort2)

testData_mort2$Mortality <- test_mort2$death_yn

# # See the structure of the new dataset
str(testData_mort2)


```



# Logistic Reg

```{r}
library(glmnet)
# Set the seed for reproducibility
set.seed(100)


my_glmnet <- getModelInfo("glmnet") %>% magrittr::extract2("glmnet")
my_glmnet$grid <- function (x, y, len = NULL, search = "grid") {
  if (search == "grid") {
    numLev <- if (is.character(y) | is.factor(y)) 
      length(levels(y))
    else NA
    if (!is.na(numLev)) {
      fam <- ifelse(numLev > 2, "multinomial", "binomial")
    }
    else fam <- "gaussian"
    init <- glmnet(as.matrix(x), y, family = fam, nlambda = 52, alpha = 0.5)
    lambda <- unique(init$lambda)
    lambda <- lambda[-c(1, length(lambda))]
    l_seq <- seq(1, length(lambda), length = len) %>% round %>% unique
    lambda <- lambda[l_seq]
    out <- expand.grid(alpha = seq(0.1, 1, length = len), 
                       lambda = lambda)
  }
  else {
    out <- data.frame(alpha = runif(len, min = 0, 1), lambda = 2^runif(len, 
                                                                       min = -10, 3))
  }
  out
}


#levels(trainData_hosp$Hosp) <- c("Yes", "No")

# Train the model using randomForest and predict on the training data itself.
model_logistic = train(Mortality ~ ., data=trainData_mort2, method= my_glmnet)

plot(model_logistic, main="Model Accuracies with LR")

varimp_lr <- varImp(model_logistic)

varimp_lr

#plot(varimp_lr, main="Variable Importance with LR")

#levels(testData_hosp$Hosp) <- c("Yes", "No")

predicted = predict(model_logistic,testData_mort2[,c(1:50)])

perf_lr = confusionMatrix(reference = testData_mort2$Mortality, data = predicted, mode='everything')

perf_lr


#roc_hosp_lr = roc(as.integer(testData_hosp$Hosp),as.integer(predicted))
#plot(roc_hosp_lr)

#auc_hosp_lr = auc(roc_hosp_lr)
#auc_hosp_lr

```

# FIT CONTROL

```{r}
fitControl <- trainControl(
    method = 'cv',                   # k-fold cross validation
    number = 5,                      # number of folds
    savePredictions = 'final',       # saves predictions for optimal tuning parameter
    classProbs = T,                  # should class probabilities be returned
    summaryFunction=twoClassSummary  # results summary function
) 
```

#### SVM 

```{r}

# Training the model on training data
levels(trainData_mort2$Mortality) <- c("No", "Yes")
model_svmRadial = train(Mortality ~ ., data=trainData_mort2, method='svmRadial', tuneLength=15, trControl = fitControl)
#model_svmRadial

# Testing it on test data

varimp_svm <- varImp(model_svmRadial)

varimp_svm

#plot(varimp_svm, main="Variable Importance with SVM")

levels(testData_mort2$Mortality) <- c("No", "Yes")

predicted = predict(model_svmRadial,testData_mort2[,c(1:50)])

perf_svm = confusionMatrix(reference = testData_mort2$Mortality, data = predicted, mode='everything')

perf_svm



```


#### Random Forest


```{r}
# Training the model on training data
levels(trainData_mort2$Mortality) <- c("No", "Yes")
model_rf = train(Mortality ~ ., data=trainData_mort2, method='rf', tuneLength= 10, trControl = fitControl)
#model_rf

# Testing it on test data

varimp_rf <- varImp(model_rf)

varimp_rf

#plot(varimp_rf, main="Variable Importance with Random Forest")

levels(testData_mort2$Mortality) <- c("No", "Yes")

predicted = predict(model_rf,testData_mort2[,c(1:50)])

perf_rf = confusionMatrix(reference = testData_mort2$Mortality, data = predicted, mode='everything')

perf_rf

```


### XG Boost Dart 

```{r}
# Training the model on training data
levels(trainData_mort2$Mortality) <- c("No", "Yes")
model_xgbdart = train(Mortality ~ ., data=trainData_mort2, method='xgbDART', tuneLength= 5, trControl = fitControl)
#model_xgbdart

# Testing it on test data

varimp_xgbdart <- varImp(model_xgbdart)

varimp_xgbdart

#plot(varimp_xgbdart, main="Variable Importance with XG Boost")

levels(testData_mort2$Mortality) <- c("No", "Yes")

predicted = predict(model_xgbdart,testData_mort2[,c(1:50)])

perf_xgbdart = confusionMatrix(reference = testData_mort2$Mortality, data = predicted, mode='everything')

perf_xgbdart


```

