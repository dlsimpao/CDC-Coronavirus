---
title: "Classification - Final Project"
author: 'Falak Shah (SID: 914663151)'
date: "5/23/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(timeDate)
library(ggplot2)
library(plotly)
library(hrbrthemes)
#install.packages("hrbrthemes")
#install.packages("tseries")
library(tseries)
#install.packages("ggmap")
library(ggmap)
library(mltools)
library(data.table)
library(caret)
#install.packages("faux")
library(faux)
#install.packages("DataExplorer")
library(DataExplorer)
#install.packages('e1071', dependencies=TRUE)
library(e1071)

```

# Loading the data
```{r}
cdc_classification = read.csv('cdc_filtered_encoded.csv')
cdc_classification = cdc_classification %>% filter(sex != "Missing")
cdc_classification = cdc_classification[,2:32]
cdc_classification[,c(1,2,4,7:31)] = lapply(cdc_classification[,c(1,2,4,7:31)],factor) 

```



### Testing on subset

```{r}
set.seed(100)
subset_classification <- cdc_classification %>% sample_frac(0.05)

# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(subset_classification$hosp_yn, p=0.75, list=FALSE)

# Step 2: Create the training  dataset
trainData <- subset_classification[trainRowNumbers,]

# Step 3: Create the test dataset
testData <- subset_classification[-trainRowNumbers,]



```






```{r}

train_hosp <- trainData %>% select(c("current_status", "sex", "hosp_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

test_hosp <- testData %>% select(c("current_status", "sex", "hosp_yn", "hc_work_yn", "pna_yn", "abxchest_yn"    ,"acuterespdistress_yn", "fever_yn", "sfever_yn", "chills_yn", "myalgia_yn", "runnose_yn", "sthroat_yn",               
"cough_yn", "sob_yn", "nauseavomit_yn", "headache_yn", "abdom_yn", "diarrhea_yn", "medcond_yn","age_group"))

#head(train_hosp)



# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_hosp <- dummyVars(hosp_yn ~ . , data=train_hosp)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_hosp <- predict(dummies_model_hosp, newdata = train_hosp)

# # Convert to dataframe
trainData_hosp <- data.frame(trainData_hosp)

trainData_hosp$Hosp <- train_hosp$hosp_yn

# # See the structure of the new dataset
str(trainData_hosp)


# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model_hosp_test <- dummyVars(hosp_yn ~ . , data=test_hosp)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
testData_hosp <- predict(dummies_model_hosp_test, newdata = test_hosp)

# # Convert to dataframe
testData_hosp <- data.frame(testData_hosp)

testData_hosp$Hosp <- test_hosp$hosp_yn

# # See the structure of the new dataset
str(testData_hosp)


```




```{r}
library(glmnet)
# Set the seed for reproducibility
set.seed(100)


my_glmnet <- getModelInfo("glmnet") %>% magrittr::extract2("glmnet")
my_glmnet$grid <- function (x, y, len = NULL, search = "grid") {
  if (search == "grid") {
    numLev <- if (is.character(y) | is.factor(y)) 
      length(levels(y))
    else NA
    if (!is.na(numLev)) {
      fam <- ifelse(numLev > 2, "multinomial", "binomial")
    }
    else fam <- "gaussian"
    init <- glmnet(as.matrix(x), y, family = fam, nlambda = 52, alpha = 0.5)
    lambda <- unique(init$lambda)
    lambda <- lambda[-c(1, length(lambda))]
    l_seq <- seq(1, length(lambda), length = len) %>% round %>% unique
    lambda <- lambda[l_seq]
    out <- expand.grid(alpha = seq(0.1, 1, length = len), 
                       lambda = lambda)
  }
  else {
    out <- data.frame(alpha = runif(len, min = 0, 1), lambda = 2^runif(len, 
                                                                       min = -10, 3))
  }
  out
}


# Train the model using randomForest and predict on the training data itself.
model_logistic = train(Hosp ~ ., data=trainData_hosp, method= my_glmnet)

plot(model_logistic, main="Model Accuracies with LR")

varimp_lr <- varImp(model_logistic)

varimp_lr

#plot(varimp_lr, main="Variable Importance with LR")

predicted = predict(model_logistic,testData_hosp[,c(1:47)])

perf_lr = confusionMatrix(reference = testData_hosp$Hosp, data = predicted, mode='everything')

perf_lr


```

```{r}
fitControl <- trainControl(
    method = 'cv',                   # k-fold cross validation
    number = 5,                      # number of folds
    savePredictions = 'final',       # saves predictions for optimal tuning parameter
    classProbs = T,                  # should class probabilities be returned
    summaryFunction=twoClassSummary  # results summary function
) 
```


#### SVM 

```{r}

# Training the model on training data
levels(trainData_hosp$Hosp) <- c("No", "Yes")
model_svmRadial = train(Hosp ~ ., data=trainData_hosp, method='svmRadial', tuneLength=15, trControl = fitControl)
#model_svmRadial

# Testing it on test data

varimp_svm <- varImp(model_svmRadial)

varimp_svm

#plot(varimp_svm, main="Variable Importance with SVM")

levels(testData_hosp$Hosp) <- c("No", "Yes")

predicted = predict(model_svmRadial,testData_hosp[,c(1:47)])

perf_svm = confusionMatrix(reference = testData_hosp$Hosp, data = predicted, mode='everything')

perf_svm



```


#### Random Forest


```{r}
# Training the model on training data
levels(trainData_hosp$Hosp) <- c("No", "Yes")
model_rf = train(Hosp ~ ., data=trainData_hosp, method='rf', tuneLength= 10, trControl = fitControl)
#model_rf

# Testing it on test data

varimp_rf <- varImp(model_rf)

varimp_rf

#plot(varimp_rf, main="Variable Importance with Random Forest")

levels(testData_hosp$Hosp) <- c("No", "Yes")

predicted = predict(model_rf,testData_hosp[,c(1:47)])

perf_rf = confusionMatrix(reference = testData_hosp$Hosp, data = predicted, mode='everything')

perf_rf

```


### XG Boost Dart 

```{r}
# Training the model on training data
levels(trainData_hosp$Hosp) <- c("No", "Yes")
model_xgbdart = train(Hosp ~ ., data=trainData_hosp, method='xgbDART', tuneLength= 5, trControl = fitControl)
#model_xgbdart

# Testing it on test data

varimp_xgbdart <- varImp(model_xgbdart)

varimp_xgbdart

#plot(varimp_xgbdart, main="Variable Importance with XG Boost")

levels(testData_hosp$Hosp) <- c("No", "Yes")

predicted = predict(model_xgbdart,testData_hosp[,c(1:47)])

perf_xgbdart = confusionMatrix(reference = testData_hosp$Hosp, data = predicted, mode='everything')

perf_xgbdart



```
